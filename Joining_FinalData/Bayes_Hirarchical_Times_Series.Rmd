---
title: "CausalImpact Analysis on Entropy After Agent Gaze"
author: "Your Name"
date: "`r Sys.Date()`"
output: html_document
---

# Bayesian Analysis of Entropy Over Time

## Introduction

In this analysis, we implement a **Bayesian hierarchical model** to examine how entropy varies after gazing on an Agent. This approach allows us to quantify uncertainty and incorporate prior knowledge, providing a more robust alternative to frequentist methods, particularly for complex hierarchical structures.

We use the **brms** package, which provides an intuitive interface to fit Bayesian models using **Stan**. The model accounts for:

- **Fixed effects**: The experimental **Condition (pre-post gaze)**, which captures differences before and after the gaze.
- **Time-dependent effects**: A **smooth function of Event_Index**, modeled using splines (`s(Event_Index)`).
- **Random effects**: Nested participant and session effects (`(1 | Participant/Session)`), which account for variability within subjects.

The dependent variable, **Entropy**, represents the measure of uncertainty or variability in our system, and we assume it follows a **Gaussian distribution**.

---

# **1. Load Required Libraries**
Before starting the analysis, we load the necessary libraries for data manipulation, visualization, time-series processing, and Bayesian modeling.

```{r load_libraries, message=FALSE, warning=FALSE}

# Load necessary libraries
library(CausalImpact)  # For causal inference and impact analysis
library(dplyr)         # Data manipulation
library(ggplot2)       # Visualization
library(readr)         # Reading CSV files
library(lubridate)     # Working with dates and timestamps
library(zoo)           # Required for time-series data
library(tidyverse)     # Meta-package (includes dplyr, ggplot2, tidyr, etc.)
library(tidyr)         # Data tidying functions
library(brms)          # Bayesian regression modeling via Stan
library(stringr)       # String manipulation (used for ID extraction)

```

# **2. Load Data from Experiment 1

We start by retrieving the entropy results stored in CSV files and merging them into a single dataframe..


```{r message=FALSE, warning=FALSE, echo=TRUE}
# Load necessary packages
library(dplyr)
library(readr)
library(stringr)

# Define directories for each experiment
exp1_dir <- "/Volumes/TwoTeras/0_Experiment_1/Entropy_Results/Window/entropy_results/CausalImpact/"
exp2_dir <- "/Volumes/TwoTeras/1_Experiment_2/Entropy_Results/Window/entropy_results/CausalImpact/"
labels <- read.csv("/Volumes/TwoTeras/Resources/AgentCoordinates_short_exp2.csv")

# List all CSV files in each directory
exp1_files <- list.files(exp1_dir, pattern = "*.csv", full.names = TRUE)
exp2_files <- list.files(exp2_dir, pattern = "*.csv", full.names = TRUE)

# Function to extract first two digits from Collider_Name
extract_first_two_digits <- function(name) {
  num <- str_extract(name, "^\\d{2}")  # Extract first two digits
  as.numeric(num)  # Convert to numeric
}

# Load and process Experiment 1
exp1_list <- lapply(exp1_files, read_csv)
Experiment1 <- bind_rows(exp1_list) %>%
  mutate(Experiment = 1,
         Collider_Number = extract_first_two_digits(Collider_Name),  # Extract first two digits
         Building = ifelse(Collider_Number <= 28, "Public", "Residential"),  # Assign 'Building'
         Building_Effect = ifelse(Building == "Public", 0.5, -0.5))  # Apply effect coding

# Ensure Collider_Number exists before removing it
if ("Collider_Number" %in% colnames(Experiment1)) {
  Experiment1 <- Experiment1 %>% dplyr::select(-Collider_Number)
}

# Load and process Experiment 2
exp2_list <- lapply(exp2_files, read_csv)
Experiment2 <- bind_rows(exp2_list) %>%
  mutate(Experiment = 2,
         Collider_Number = extract_first_two_digits(Collider_Name)) %>%
  left_join(labels, by = c("Collider_Name" = "AvatarName")) %>%  # Match agent names
  mutate(Building = ifelse(Context == TRUE, "Public", "Residential"),
         Building_Effect = ifelse(Building == "Public", 0.5, -0.5)) %>%  # Apply effect coding
  dplyr::select(all_of(names(Experiment1)))  # Ensure same columns as Experiment1

# Combine both experiments
df_combined <- bind_rows(Experiment1, Experiment2)

# Check final dataset structure
glimpse(df_combined)

# View first rows to confirm changes
head(df_combined)
```



# 3. Data Transformation

Before fitting the Bayesian model, we prepare additional categorical variables:
- **Splitting `Participant_ID`** into separate `ID` (participant identifier) and `Session` (session number).
- **Defining `Agent_Type`** as **Active (Cma)** or **Passive (Sa)** based on `Collider_Name`.
- **Applying Effect Coding**:  
  - `-0.5` for **Passive** agents (Sa).  
  - `0.5` for **Active** agents (Cma).

```{r message=FALSE, warning=FALSE, echo=TRUE}
# Load necessary library for string manipulation


# Split Participant_ID into ID and Session
df_combined <- df_combined %>%
  mutate(
    ID = str_extract(Participant_ID, "^[^_]+"),  # Extract part before '_'
    Session = str_extract(Participant_ID, "(?<=_).*")  # Extract part after '_'
  )

# Create Agent_Type based on Collider_Name
df_combined <- df_combined %>%
  mutate(
    Agent_Type = ifelse(str_detect(Collider_Name, "Cma"), "Active", "Passive"),
    Agent_Effect = ifelse(Agent_Type == "Active", 0.5, -0.5)  # Effect coding
  )

# Convert new variables to factors
df_combined <- df_combined %>%
  mutate(
    ID = as.factor(ID),
    Session = as.factor(Session),
    Agent_Type = as.factor(Agent_Type)
  )

df_combined <- df_combined %>%
  mutate(
    # Define Match (Congruency) using the original rule
    Match = ifelse((Experiment == 1 & Agent_Effect == 0.5), 0.5, -0.5),
    
    # Effect coding for Experiment
    Experiment = case_when(
      Experiment == 1 ~ -0.5,
      Experiment == 2 ~ 0.5,
      TRUE ~ NaN  # Should never occur but ensures robustness
    )
  )

# Create Event_Index before reshaping
df_combined <- df_combined %>%
  arrange(ID, Session, Gaze_Time) %>%  # Ensure correct chronological order
  group_by(ID, Session) %>%
  mutate(Event_Index = row_number()) %>%  # Assigns a single index per gaze event
  ungroup()



# Display first few rows to verify changes
head(df_combined)

```

# 4. Reshape Data for Bayesian Analysis

To properly model entropy as a function of **time and condition**, we need to transform the dataset from **wide** to **long format**. This ensures that:
- **Entropy measurements** are stored in a single column.
- **`Condition`** (Pre vs. Post) is treated as a categorical variable.
- **Effect coding** is applied:
  - `-0.5` for **Pre_Entropy** (Before).
  - `0.5` for **Post_Entropy** (After).

This format is essential for **Bayesian hierarchical modeling**, where we want to assess changes in entropy across conditions while accounting for random effects.

```{r message=FALSE, warning=FALSE, echo=TRUE}
# Reshape the data into long format
df_long <- df_combined %>%
  pivot_longer(
    cols = c(Pre_Entropy, Post_Entropy),
    names_to = "Pre_Post",
    values_to = "Entropy"
  ) %>%
  mutate(
    # Convert Condition to a factor
    Pre_Post = factor(Pre_Post, levels = c("Pre_Entropy", "Post_Entropy")),
    
    # Apply effect coding: -0.5 for Pre_Entropy, 0.5 for Post_Entropy
    Pre_Post_Effect = ifelse(Pre_Post == "Pre_Entropy", -0.5, 0.5)
  )

# Display first few rows to verify changes
head(df_long)
```
```{r}
ggplot(df_long, aes(x = Entropy, fill = Pre_Post)) +
  geom_density(alpha = 0.5) +
  theme_minimal()

```
```{r}
qqnorm(df_long$Entropy)
qqline(df_long$Entropy, col = "red")
```

```{r}
bayesian_time_model <- brm(
  Entropy ~ Pre_Post_Effect + s(Event_Index, k = 5summar) + (1 | ID/Session),
  data = df_long,
  family = gaussian(),
  cores = parallel::detectCores(),
  chains = 4, iter = 4000, warmup = 1000,
  save_pars = save_pars(all = TRUE)
)

```

```{r}
bayes_post_model <- brm(
  Post_Entropy ~  Building_Effect + Agent_Effect + Match + s(Event_Index, k = 5) + (1 | ID/Session),
  data = df_combined,
  family = gaussian(),
  cores = 4,
  chains = 4, iter = 4000, warmup = 1000,
  save_pars = save_pars(all = TRUE)
)
```



```{r}
library(data.table)

# Extract Bayesian model estimates
model_df <- data.table(coef(summary(bayesian_time_model)), keep.rownames = "term")

# Save to CSV
write.csv(model_df, "/Volumes/TwoTeras/Resources/bayes_model.csv", row.names = FALSE)

```


```{r}
# Extract posterior draws
posterior_samples <- as.data.frame(posterior_samples(bayesian_time_model))

# Compute correlations between fixed effect estimates
cor(posterior_samples %>% select(starts_with("b_")))

```

```{r}
conditional_effects(bayesian_time_model)
```

```{r}
library(bayesplot)

# Plot posterior distributions of fixed effects
mcmc_plot(bayesian_time_model, pars = c("b_Pre_Post_Effect", "b_Building_Effect", "b_Agent_Effect", "b_Match", "b_sEvent_Index_1"))

```

```{r}
library(ggplot2)
ce <- conditional_effects(bayesian_time_model)
ggplot(ce$Event_Index, aes(x = Event_Index, y = estimate__)) +
  geom_line() +
  geom_ribbon(aes(ymin = lower__, ymax = upper__), alpha = 0.2) +
  labs(title = "Conditional Effect of Event Index on Entropy")



```

```{r}
# ðŸ“Œ Step 1: Check if the model exists
if (!exists("bayesian_time_model")) {
  message("Model not found. Loading it from file...")
  load("bayesian_time_model.RData")  # Ensure you saved it earlier
}

# ðŸ“Œ Step 2: Create a new dataset for predictions with numeric effect coding
new_data <- expand.grid(
  Event_Index = seq(min(df_long$Event_Index), max(df_long$Event_Index), length.out = 100),
  Pre_Post_Effect = c(-0.5, 0.5),  # Effect-coded numeric values (-0.5 for Pre, 0.5 for Post)
  Building_Effect = mean(df_long$Building_Effect, na.rm = TRUE),  # Hold other variables constant
  Agent_Effect = mean(df_long$Agent_Effect, na.rm = TRUE),
  Match = mean(df_long$Match, na.rm = TRUE),
  ID = "new_ID",  # Dummy placeholder to avoid NA issue
  Session = "new_Session"  # Dummy placeholder to avoid NA issue
)

# ðŸ“Œ Step 3: Get predictions from the model (Allow New Random Effect Levels)
preds <- as.data.frame(predict(bayesian_time_model, newdata = new_data, re.form = NA, allow_new_levels = TRUE))

# Rename columns to match expected format
colnames(preds) <- c("predicted_entropy", "lower", "upper")

# Add predictions to dataset
new_data <- cbind(new_data, preds)

# ðŸ“Œ Step 4: Convert Pre_Post_Effect back to descriptive labels for plotting
new_data$Pre_Post_Label <- ifelse(new_data$Pre_Post_Effect == -0.5, "Pre", "Post")

new_data <- new_data %>%
  select(Event_Index, Pre_Post_Effect, Building_Effect, Agent_Effect, Match, 
         ID, Session, predicted_entropy, lower, upper, Pre_Post_Label) %>%
  distinct()  # Remove duplicates if any exist
```

```{r}

```

```{r}
# ðŸ“Œ Step 5: Plot the predictions using ggplot2
library(ggplot2)
library(dplyr)

# Define the bins and ensure ordering
new_data <- new_data %>%
  mutate(Event_Bin = factor(case_when(
    Event_Index >= 0 & Event_Index < 3  ~ "0-3",
    Event_Index >= 3 & Event_Index < 10 ~ "3-10",
    Event_Index >= 10 & Event_Index < 20 ~ "10-20",
    Event_Index >= 20 ~ ">20"
  ), levels = c("0-3", "3-10", "10-20", ">20")))  # Ordered bins

# Ensure "Pre" is plotted before "Post"
new_data <- new_data %>%
  mutate(Pre_Post_Label = factor(Pre_Post_Label, levels = c("Pre", "Post")))  # Ordered factor

# Summarize data by bin and Pre_Post_Label with standard error calculation
binned_data <- new_data %>%
  group_by(Event_Bin, Pre_Post_Label) %>%
  summarise(
    mean_entropy = mean(predicted_entropy, na.rm = TRUE),
    se_entropy = sd(predicted_entropy, na.rm = TRUE) / sqrt(n()),
    .groups = 'drop'
  )

# Plot with error bars and value annotations
ggplot(binned_data, aes(x = Event_Bin, y = mean_entropy, fill = Pre_Post_Label)) +
  geom_col(position = position_dodge(width = 0.7), width = 0.6) +  # Bar plot with adjusted width
  geom_errorbar(aes(ymin = mean_entropy - se_entropy, ymax = mean_entropy + se_entropy),
                position = position_dodge(width = 0.7), width = 0.2) +  # Error bars
  geom_text(aes(label = round(mean_entropy, 2)), 
            position = position_dodge(width = 0.7), 
            vjust = -0.5, size = 4) +  # Value annotations
  labs(title = "Binned Effect of number of agent encounters on Entropy",
       subtitle = "Increased entropy after agent encounters in free exploration",
       x = "Number of agent encounters", y = "Mean Predicted Entropy") +
  theme_minimal() +
  scale_fill_manual(values = c("blue", "red"), labels = c("Pre", "Post"))  # Ensuring colors and labels match



```
```{r}
ggplot(new_data, aes(x = Event_Index, y = predicted_entropy, color = Pre_Post_Label)) +
  geom_line(size = 1) +  # Smooth line for Pre vs. Post
  labs(title = "Effect of Event Index on Entropy",
       subtitle = "Comparison of Pre and Post Conditions",
       x = "Event Index (Time)", y = "Predicted Entropy") +
  theme_minimal() +
  scale_color_manual(values = c("blue", "red")) +
  scale_fill_manual(values = c("blue", "red"))
```


```{r}
# Null Model: No predictors, only random effects
bayesian_model_null <- brm(
  Entropy ~ 1 + (1 | ID/Session),
  data = df_long,
  family = gaussian(),
  core=4,
  chains = 4, iter = 4000, warmup = 1000,
  save_pars = save_pars(all = TRUE)
)

```



```{r}
library(bridgesampling)

bridge_full <- bridge_sampler(bayesian_time_model, method = "warp3", repetitions = 50)
bridge_null <- bridge_sampler(bayesian_model_null, method = "warp3", repetitions = 50)

bayes_factor_full_vs_null <- bayes_factor(bridge_full, bridge_null)

print(bayes_factor_full_vs_null)



```


```{r}
library(BayesFactor)

# Full model (all predictors)
bf_full <- lmBF(Entropy ~ Pre_Post_Effect + Building_Effect + Agent_Effect + Match + Event_Index, 
                data = df_long_clean, 
                whichRandom = c("Participant_ID", "Session"))

# Models removing one term at a time
bf_no_PrePost <- lmBF(Entropy ~ Building_Effect + Agent_Effect + Match + Event_Index, 
                       data = df_long_clean, 
                       whichRandom = c("Participant_ID", "Session"))

bf_no_Building <- lmBF(Entropy ~ Pre_Post_Effect + Agent_Effect + Match + Event_Index, 
                        data = df_long_clean, 
                        whichRandom = c("Participant_ID", "Session"))

bf_no_Agent <- lmBF(Entropy ~ Pre_Post_Effect + Building_Effect + Match + Event_Index, 
                     data = df_long_clean, 
                     whichRandom = c("Participant_ID", "Session"))

bf_no_Match <- lmBF(Entropy ~ Pre_Post_Effect + Building_Effect + Agent_Effect + Event_Index, 
                     data = df_long_clean, 
                     whichRandom = c("Participant_ID", "Session"))

bf_no_EventIndex <- lmBF(Entropy ~ Pre_Post_Effect + Building_Effect + Agent_Effect + Match, 
                          data = df_long_clean, 
                          whichRandom = c("Participant_ID", "Session"))

# Compute individual Bayes Factors
bf_PrePost <- bf_full / bf_no_PrePost
bf_Building <- bf_full / bf_no_Building
bf_Agent <- bf_full / bf_no_Agent
bf_Match <- bf_full / bf_no_Match
bf_EventIndex <- bf_full / bf_no_EventIndex

# Print results
bf_PrePost
bf_Building
bf_Agent
bf_Match
bf_EventIndex

```
```{r}
library(ggplot2)

# Create a dataframe with predictors and their Bayes Factors
bf_data <- data.frame(
  Predictor = c("Pre_Post_Effect", "Building_Effect", "Agent_Effect", "Match", "Event_Index"),
  BayesFactor = c(1.15e+33, 0.0387, 403.70, 0.0387, 2.01e+10)  # Use your computed BF values
)

# Apply log transformation (to make the scale readable)
bf_data$LogBF <- log10(bf_data$BayesFactor)

# Define colors based on importance
bf_data$Category <- ifelse(bf_data$BayesFactor > 30, "Strong Evidence", 
                      ifelse(bf_data$BayesFactor > 3, "Moderate Evidence", 
                      ifelse(bf_data$BayesFactor > 1, "Weak Evidence", "Against Inclusion")))

# Define custom colors
color_palette <- c("Strong Evidence" = "darkred", "Moderate Evidence" = "orange", 
                   "Weak Evidence" = "yellow", "Against Inclusion" = "blue")

# âœ… 2. Plot the Bayes Factors
ggplot(bf_data, aes(x = reorder(Predictor, LogBF), y = LogBF, fill = Category)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = round(BayesFactor, 2)), hjust = 1.2, color = "white", size = 4) +
  scale_fill_manual(values = color_palette) +
  labs(title = "Bayes Factors for Predictors",
       subtitle = "Log-transformed Bayes Factors (BF > 1 favors inclusion, BF < 1 favors exclusion)",
       x = "Predictor",
       y = "Log10(Bayes Factor)") +
  theme_minimal() +
  coord_flip()  # Flips the axes for readability

```




```{r }

# Ensure data is sorted by participant and event order
df <- df %>%
  group_by(Participant_ID) %>%  
  arrange(Gaze_Time) %>%  
  mutate(Event_Index = row_number()) %>%  
  ungroup()

# Function to ensure valid pre-periods
run_causal_impact <- function(df_part) {
  # Convert entropy to a zoo time series
  time_series <- zoo(df_part$Post_Entropy, df_part$Event_Index)
  
  # Set minimum pre-period size
  min_pre_period_size <- max(3, floor(nrow(df_part) / 3))  # At least 3 points
  
  # Ensure post-period starts at a reasonable place
  pre_period_end <- min_pre_period_size
  post_period_start <- pre_period_end + 1
  
  # Define pre/post periods dynamically
  pre_period <- c(1, pre_period_end)
  post_period <- c(post_period_start, nrow(df_part))
  
  # Run CausalImpact only if valid pre-period
  if (length(time_series) > pre_period_end + 2) {
    impact <- CausalImpact(time_series, pre_period, post_period)
    return(impact)
  } else {
    return(NULL)  # Skip small datasets
  }
}

# Run CausalImpact per participant and store results
results_list <- list()
for (participant in unique(df$Participant_ID)) {
  df_part <- df %>% filter(Participant_ID == participant)
  
  impact <- run_causal_impact(df_part)
  
  if (!is.null(impact)) {
    results_list[[participant]] <- summary(impact)
    
    # Save plot
    png(paste0("CausalImpact_", participant, ".png"), width=800, height=600)
    plot(impact)
    dev.off()
  }
}

# Print all participant results
results_list

```
```{r}

```


```{r }
# Run CausalImpact per participant-session
results_list <- list()

for (participant in unique(df$Participant_ID)) {
  df_part <- df %>% filter(Participant_ID == participant)  # Filter participant data
  time_series_part <- zoo(df_part$Post_Entropy, df_part$Event_Index)

  # Adjust pre/post periods for this participant
  pre_period_part <- c(1, floor(nrow(df_part) / 2))
  post_period_part <- c(floor(nrow(df_part) / 2) + 1, nrow(df_part))

  impact <- CausalImpact(time_series_part, pre_period_part, post_period_part)

  # Store results
  results_list[[participant]] <- summary(impact)
  
  # Optional: Save each plot separately
  png(paste0("CausalImpact_", participant, ".png"), width=800, height=600)
  plot(impact)
  dev.off()
}

# Print all participant results
results_list
```
```{r}

```

# ** Condition Model: Pre-Post
To apply CausalImpact, we first convert the entropy values into a time series object.

```{r}
library(lme4)
library(lmerTest)  # For p-values
library(tidyr)
library(dplyr)  # Ensure dplyr is loaded for pipes (%>%)

# Assuming Participant_ID is in the format "1131_1"




# Fit a Linear Mixed Model with random intercepts for participants and sessions
model_nested <- lmer(Entropy ~ Condition + (1 | Participant/Session), data = df_long)

# Print model summary
summary(model_nested)
```
```{r}
library(brms)

bayesian_time_model <- brm(
  Entropy ~ Condition + s(Event_Index) + (1 | Participant/Session),  
  data = df_long,
  family = gaussian(),
  prior = c(
    prior(normal(0, 0.1), class = "b"),  # Prior for fixed effects
    prior(normal(0, 0.1), class = "Intercept"),  # Prior for intercept
    prior(cauchy(0, 1), class = "sd"),  # Random effect priors
    prior(cauchy(0, 1), class = "sds")  # Prior for time smoothing
  ),
  iter = 4000, warmup = 1000, chains = 4, cores = 4,
  control = list(adapt_delta = 0.95)
)

summary(bayesian_time_model)

```

# **Null model
```{r}
library(brms)
library(BayesFactor)  # For direct Bayes Factor computation

# Full Model: Includes Condition (Pre vs. Post)
bayesian_model_full <- brm(
  Entropy ~ Condition + (1 | Participant/Session),  
  data = df_long,
  family = gaussian(),
  prior = c(
    prior(normal(0, 0.1), class = "b"),
    prior(normal(0, 0.1), class = "Intercept"),
    prior(cauchy(0, 1), class = "sd")
  ),
  iter = 4000, warmup = 1000, chains = 4, cores = 4,
  control = list(adapt_delta = 0.95)
)

# Null Model: Excludes Condition
bayesian_model_null <- brm(
  Entropy ~ 1 + (1 | Participant/Session),  # No Condition effect
  data = df_long,
  family = gaussian(),
  prior = c(
    prior(normal(0, 0.1), class = "Intercept"),
    prior(cauchy(0, 1), class = "sd")
  ),
  iter = 4000, warmup = 1000, chains = 4, cores = 4,
  control = list(adapt_delta = 0.95)
)

# Compute Bayes Factor
bayes_factor <- bayes_factor(bayesian_model_full, bayesian_model_null)
print(bayes_factor)

```
# Entropy changes over time AR t-1 
```{r}
bayesian_ar1_model <- brm(
  Entropy ~ Condition + ar(Time_Index, p = 1) + (1 | Participant/Session),  
  data = df_long,
  family = gaussian(),
  prior = c(
    prior(normal(0, 0.1), class = "b"),
    prior(normal(0, 0.1), class = "Intercept"),
    prior(cauchy(0, 1), class = "sd"),
    prior(normal(0, 1), class = "ar")  # Prior for AR(1) effect
  ),
  iter = 4000, warmup = 1000, chains = 4, cores = 4,
  control = list(adapt_delta = 0.95)
)

summary(bayesian_ar1_model)

```
# Gaussian Process Priors (Smooth Entropy Changes Over Time)
If entropy doesnâ€™t change abruptly, we can use Gaussian Process (GP) priors to smooth the effect of time.
```{r}
bayesian_gp_model <- brm(
  Entropy ~ Condition + gp(Time_Index) + (1 | Participant/Session),  
  data = df_long,
  family = gaussian(),
  prior = c(
    prior(normal(0, 0.1), class = "b"),
    prior(normal(0, 0.1), class = "Intercept"),
    prior(cauchy(0, 1), class = "sd"),
    prior(cauchy(0, 1), class = "gp")  # GP prior for time smoothness
  ),
  iter = 4000, warmup = 1000, chains = 4, cores = 4,
  control = list(adapt_delta = 0.95)
)

summary(bayesian_gp_model)
```
# Compare the models 
```{r}
loo_compare(bayesian_model_full, bayesian_ar1_model, bayesian_gp_model)

```

# ** 5. Run CausalImpact Analysis
```{r}
# Run CausalImpact analysis
df$Post_Entropy <- as.numeric(df$Post_Entropy)
time_series <- ts(df$Post_Entropy)

impact <- CausalImpact(time_series, pre_period, post_period)

# Print results
summary(impact)

```
# **6. Visualize the Impact
The following plot displays the predicted vs. actual entropy values, along with the estimated impact of agent gaze.

```{r}
# Save the summary as a text file
writeLines(capture.output(summary(impact)), "CausalImpact_Results.txt")

# Save the plot
png("CausalImpact_Plot.png", width=800, height=600)
plot(impact)
dev.off()

```



