---
title: "CausalImpact Analysis on Entropy After Agent Gaze"
author: "Your Name"
date: "`r Sys.Date()`"
output: html_document
---

# Bayesian Analysis of Entropy Over Time

## Introduction

In this analysis, we implement a **Bayesian hierarchical model** to examine how entropy varies after gazing on an Agent. This approach allows us to quantify uncertainty and incorporate prior knowledge, providing a more robust alternative to frequentist methods, particularly for complex hierarchical structures.

We use the **brms** package, which provides an intuitive interface to fit Bayesian models using **Stan**. The model accounts for:

- **Fixed effects**: The experimental **Condition (pre-post gaze)**, which captures differences before and after the gaze.
- **Time-dependent effects**: A **smooth function of Event_Index**, modeled using splines (`s(Event_Index)`).
- **Random effects**: Nested participant and session effects (`(1 | Participant/Session)`), which account for variability within subjects.

The dependent variable, **Entropy**, represents the measure of uncertainty or variability in our system, and we assume it follows a **Gaussian distribution**.

---

# **1. Load Required Libraries**
Before starting the analysis, we load the necessary libraries for data manipulation, visualization, time-series processing, and Bayesian modeling.

```{r load_libraries, message=FALSE, warning=FALSE}

# Load necessary libraries
library(CausalImpact)  # For causal inference and impact analysis
library(dplyr)         # Data manipulation
library(ggplot2)       # Visualization
library(readr)         # Reading CSV files
library(lubridate)     # Working with dates and timestamps
library(zoo)           # Required for time-series data
library(tidyverse)     # Meta-package (includes dplyr, ggplot2, tidyr, etc.)
library(tidyr)         # Data tidying functions
library(brms)          # Bayesian regression modeling via Stan
library(stringr)       # String manipulation (used for ID extraction)

```

# **2. Load Data

We start by retrieving the entropy results stored in CSV files and merging them into a single dataframe..


```{r message=FALSE, warning=FALSE, echo=TRUE}


# Define directories for each experiment
exp1_dir <- "/Volumes/TwoTeras/0_Experiment_1/Entropy_Results/Window/entropy_results/CausalImpact/"
exp2_dir <- "/Volumes/TwoTeras/1_Experiment_2/Entropy_Results/Window/entropy_results/CausalImpact/"
labels <- read.csv("/Volumes/TwoTeras/Resources/AgentCoordinates_short_exp2.csv")

# List all CSV files in each directory
exp1_files <- list.files(exp1_dir, pattern = "*.csv", full.names = TRUE)
exp2_files <- list.files(exp2_dir, pattern = "*.csv", full.names = TRUE)

# Function to extract first two digits from Collider_Name
extract_first_two_digits <- function(name) {
  num <- str_extract(name, "^\\d{2}")  # Extract first two digits
  as.numeric(num)  # Convert to numeric
}

# Load and process Experiment 1
exp1_list <- lapply(exp1_files, read_csv)
Experiment1 <- bind_rows(exp1_list) %>%
  mutate(Experiment = 1,
         Collider_Number = extract_first_two_digits(Collider_Name),  # Extract first two digits
         Building = ifelse(Collider_Number <= 28, "Public", "Residential"),  # Assign 'Building'
         Building_Effect = ifelse(Building == "Public", 0.5, -0.5))  # Apply effect coding

# Ensure Collider_Number exists before removing it
if ("Collider_Number" %in% colnames(Experiment1)) {
  Experiment1 <- Experiment1 %>% dplyr::select(-Collider_Number)
}

# Load and process Experiment 2
exp2_list <- lapply(exp2_files, read_csv)
Experiment2 <- bind_rows(exp2_list) %>%
  mutate(Experiment = 2,
         Collider_Number = extract_first_two_digits(Collider_Name)) %>%
  left_join(labels, by = c("Collider_Name" = "AvatarName")) %>%  # Match agent names
  mutate(Building = ifelse(Context == TRUE, "Public", "Residential"),
         Building_Effect = ifelse(Building == "Public", 0.5, -0.5)) %>%  # Apply effect coding
  dplyr::select(all_of(names(Experiment1)))  # Ensure same columns as Experiment1

# Combine both experiments
df_combined <- bind_rows(Experiment1, Experiment2)

# Check final dataset structure
glimpse(df_combined)

# View first rows to confirm changes
head(df_combined)
```





# 3. Data Transformation

Before fitting the Bayesian model, we prepare additional categorical variables:
- **Splitting `Participant_ID`** into separate `ID` (participant identifier) and `Session` (session number).
- **Defining `Agent_Type`** as **Active (Cma)** or **Passive (Sa)** based on `Collider_Name`.
- **Applying Effect Coding**:  
  - `-0.5` for **Passive** agents (Sa).  
  - `0.5` for **Active** agents (Cma).

```{r message=FALSE, warning=FALSE, echo=TRUE}


# Combine both experiments
df_combined <- bind_rows(Experiment1, Experiment2)
# Split Participant_ID into ID and Session
df_combined <- df_combined %>%
  mutate(
    ID = str_extract(Participant_ID, "^[^_]+"),  # Extract part before '_'
    Session = str_extract(Participant_ID, "(?<=_).*")  # Extract part after '_'
  )

# Create Agent_Type and initial Agent_Effect based on Collider_Name
df_combined <- df_combined %>%
  mutate(
    Agent_Type = ifelse(str_detect(Collider_Name, "Cma"), "Active", "Passive"),
    Agent_Effect = ifelse(Agent_Type == "Active", 0.5, -0.5)  # Initial effect coding
  )


# Continue with additional effect coding and Event_Index creation
df_combined <- df_combined %>%
  mutate(
    # Define Match (Congruency) using the original rule
    Match = ifelse((Experiment == 1 & Agent_Effect == 0.5), 0.5, -0.5),

    # Effect coding for Experiment
    Experiment = case_when(
      Experiment == 1 ~ -0.5,
      Experiment == 2 ~ 0.5,
      TRUE ~ NaN
    )
  ) %>%
  arrange(ID, Session, Gaze_Time) %>%  # Chronological order
  group_by(ID, Session) %>%
  mutate(Event_Index = row_number()) %>%
  ungroup()

# Convert new variables to factors and explicitly define Agent_Type levels (Acontextual, Congruent, Incongruent)
df_combined <- df_combined %>%
  mutate(
    ID = as.factor(ID),
    Session = as.factor(Session),
    Agent_Type = factor(case_when(
      Agent_Effect == -0.5 ~ "Acontextual",
      Agent_Effect == 0.5 & Experiment == -0.5 ~ "Congruent",
      Agent_Effect == 0.5 & Experiment == 0.5 ~ "Incongruent",
      TRUE ~ NA_character_
    ), levels = c("Acontextual", "Congruent", "Incongruent"))
  )



# Display first few rows to verify changes
head(df_combined)

```
```{r}
# Define the properly scaled contrast matrix (divided by 2)
contrast_matrix <- matrix(
  c(-0.5,  0.5,  0,   # Congruent vs. Acontextual
    -0.5,  0,    0.5, # Incongruent vs. Acontextual
     0,    0.5, -0.5),# Congruent vs. Incongruent
  ncol = 3,
  byrow = TRUE
)

# Assign row and column names
rownames(contrast_matrix) <- c("Acontextual", "Congruent", "Incongruent")
colnames(contrast_matrix) <- c("Congruent_vs_Acontextual", "Incongruent_vs_Acontextual", "Congruent_vs_Incongruent")

# Apply the contrast matrix
contrasts(df_combined$Agent_Type) <- contrast_matrix
```

```{r}
library(dplyr)

# Create two subsets
df_above_60 <- df_combined %>% filter(Event_Index > 60)
df_below_60 <- df_combined %>% filter(Event_Index <= 60)

# Total number of rows
total_rows <- nrow(df_combined)

# Number of rows in each category
above_60 <- nrow(df_above_60)
below_60 <- nrow(df_below_60)

# Calculate percentages
above_60_pct <- (above_60 / total_rows) * 100
below_60_pct <- (below_60 / total_rows) * 100

# Print results
cat("Percentage of Event_Index > 60:", round(above_60_pct, 2), "%\n")
cat("Percentage of Event_Index â‰¤ 60:", round(below_60_pct, 2), "%\n")

# Check dimensions of the kept dataframes
dim(df_above_60)  # Shows dimensions of rows and columns
dim(df_below_60)  # Shows dimensions of rows and columns



```
# 4. Reshape Data for Bayesian Analysis

To properly model entropy as a function of **time and condition**, we need to transform the dataset from **wide** to **long format**. This ensures that:
- **Entropy measurements** are stored in a single column.
- **`Condition`** (Pre vs. Post) is treated as a categorical variable.
- **Effect coding** is applied:
  - `-0.5` for **Pre_Entropy** (Before).
  - `0.5` for **Post_Entropy** (After).

This format is essential for **Bayesian hierarchical modeling**, where we want to assess changes in entropy across conditions while accounting for random effects.

```{r message=FALSE, warning=FALSE, echo=TRUE}
# Reshape the data into long format
df_long <- df_combined %>%
  pivot_longer(
    cols = c(Pre_Entropy, Post_Entropy),
    names_to = "Pre_Post",
    values_to = "Entropy"
  ) %>%
  mutate(
    # Convert Condition to a factor
    Pre_Post = factor(Pre_Post, levels = c("Pre_Entropy", "Post_Entropy")),
    
    # Apply effect coding: -0.5 for Pre_Entropy, 0.5 for Post_Entropy
    Pre_Post_Effect = ifelse(Pre_Post == "Pre_Entropy", -0.5, 0.5)
  )

# Display first few rows to verify changes
df_long$Entropy[df_long$Entropy > 1] <- 1
head(df_long)
```


```{r message=FALSE, warning=FALSE, echo=TRUE}

write.csv(df_post, "/Volumes/TwoTeras/Resources/post_entropy.csv", row.names = FALSE)
```


```{r}
ggplot(df_long, aes(x = Entropy, fill = Pre_Post)) +
  geom_density(alpha = 0.5) +
  theme_minimal()

```
```{r}
qqnorm(df_long$Entropy)
qqline(df_long$Entropy, col = "red")
```
```{r}
sum0  <- sum(df_long$Entropy <= 0, na.rm = TRUE)
sum1  <- sum(df_long$Entropy >= 1, na.rm = TRUE)
n_tot <- sum(!is.na(df_long$Entropy))
c(sum0 = sum0, sum1 = sum1, pct0 = sum0/n_tot, pct1 = sum1/n_tot)

```

```{r}
# Ensure numeric
df_long$Entropy <- as.numeric(df_long$Entropy)

# Clamp to [0,1] just in case, then map to (0,1)
n <- sum(!is.na(df_long$Entropy))
df_long$Entropy <- pmin(pmax(df_long$Entropy, 0), 1)
df_long$Entropy <- (df_long$Entropy * (n - 1) + 0.5) / n

# Quick sanity check
range(df_long$Entropy, na.rm = TRUE)  # now strictly between 0 and 1
mean(df_long$Entropy <= 0 | df_long$Entropy >= 1, na.rm = TRUE)  # should be 0


```

```{r}
bayesian_time_model <- brm(
  Entropy ~ Pre_Post_Effect + s(Event_Index, k = 5) + (1 | ID) + (1 | Session),
  data   = df_long,
  family = Beta(link = "logit"),
  chains = 4, iter = 4000, warmup = 1500, cores = parallel::detectCores(),
  control = list(adapt_delta = 0.995, max_treedepth = 15),
  save_pars = save_pars(all = TRUE)
)

m0 <- brm(
  Entropy ~            s(Event_Index, k = 5) + (1 | ID) + (1 | Session),
  data   = df_long,
  family = Beta(link = "logit"),
  chains = 4, iter = 4000, warmup = 1500, cores = parallel::detectCores(),
  control = list(adapt_delta = 0.995, max_treedepth = 15),
  save_pars = save_pars(all = TRUE)
)



```





```{r}
library(data.table)

# Extract Bayesian model estimates
model_df <- data.table(coef(summary(bayesian_time_model)), keep.rownames = "term")

# Save to CSV
write.csv(model_df, "/Volumes/TwoTeras/Resources/bayes_model.csv", row.names = FALSE)

```


```{r}
# Extract posterior draws
posterior_samples <- as.data.frame(posterior_samples(bayesian_time_model))

# Compute correlations between fixed effect estimates
cor(posterior_samples %>% select(starts_with("b_")))

```

```{r}
conditional_effects(bayesian_time_model)
```

```{r}
library(bayesplot)

# Plot posterior distributions of fixed effects
mcmc_plot(bayesian_time_model, pars = c("b_Pre_Post_Effect", "b_Building_Effect", "b_Agent_Effect", "b_Match", "b_sEvent_Index_1"))

```

```{r}
library(ggplot2)
ce <- conditional_effects(bayesian_time_model)
ggplot(ce$Event_Index, aes(x = Event_Index, y = estimate__)) +
  geom_line() +
  geom_ribbon(aes(ymin = lower__, ymax = upper__), alpha = 0.2) +
  labs(title = "Conditional Effect of Event Index on Entropy")
```
```{r}
inv_logit <- brms::inv_logit_scaled

# 1) Posterior draws of fixed effects
draws <- as_draws_df(bayesian_time_model)
a0 <- draws$b_Intercept
aE <- draws$b_Pre_Post_Effect  # name may differ; check fixef() for the exact term

# 2) Pre and post means on probability scale (population-level)
mu_pre  <- inv_logit(a0 - 0.5 * aE)
mu_post <- inv_logit(a0 + 0.5 * aE)

summ <- function(x, probs=c(0.025,0.975)) c(mean=mean(x), quantile(x, probs))
pre_summ  <- summ(mu_pre)
post_summ <- summ(mu_post)

# 3) Relative increase (%)
rel_inc <- 100 * (mu_post / mu_pre - 1)
rel_summ <- summ(rel_inc)

# 4) Smooth drift: early vs late encounters
kq <- quantile(df_long$Event_Index, c(0.1, 0.9), na.rm=TRUE)
nd <- data.frame(Pre_Post_Effect = 0,  # effect-coded grand mean
                 Event_Index = as.numeric(kq),
                 ID = NA, Session = NA)
# Population-level (no group offsets)
ep <- posterior_epred(bayesian_time_model, newdata = nd, re_formula = NA)
# Columns: early, late
delta <- ep[,2] - ep[,1]
delta_summ <- summ(delta)

# 5) Random-intercept SDs
r_sd <- VarCorr(bayesian_time_model)
sd_id      <- r_sd$ID$sd[[1]]
sd_session <- r_sd$Session$sd[[1]]

# 6) Bayes factor (if you want to quote it)
m0 <- brm(
  Entropy ~ s(Event_Index, k = 5) + (1 | ID) + (1 | Session),
  data = df_long, family = Beta(link="logit"),
  chains=4, iter=4000, warmup=1000, cores=4, save_pars=save_pars(all=TRUE)
)
library(bridgesampling)
bf_val <- bf(bayesian_time_model, m0)$bf

```


```{r}
# ðŸ“Œ Step 1: Check if the model exists
if (!exists("bayesian_time_model")) {
  message("Model not found. Loading it from file...")
  load("bayesian_time_model.RData")  # Ensure you saved it earlier
}

# ðŸ“Œ Step 2: Create a new dataset for predictions with numeric effect coding
new_data <- expand.grid(
  Event_Index = seq(min(df_long$Event_Index), max(df_long$Event_Index), length.out = 100),
  Pre_Post_Effect = c(-0.5, 0.5),  # Effect-coded numeric values (-0.5 for Pre, 0.5 for Post)
  Building_Effect = mean(df_long$Building_Effect, na.rm = TRUE),  # Hold other variables constant
  Agent_Effect = mean(df_long$Agent_Effect, na.rm = TRUE),
  Match = mean(df_long$Match, na.rm = TRUE),
  ID = "new_ID",  # Dummy placeholder to avoid NA issue
  Session = "new_Session"  # Dummy placeholder to avoid NA issue
)

# ðŸ“Œ Step 3: Get predictions from the model (Allow New Random Effect Levels)
preds <- as.data.frame(predict(bayesian_time_model, newdata = new_data, re.form = NA, allow_new_levels = TRUE))

# Rename columns to match expected format
colnames(preds) <- c("predicted_entropy", "lower", "upper")

# Add predictions to dataset
new_data <- cbind(new_data, preds)

# ðŸ“Œ Step 4: Convert Pre_Post_Effect back to descriptive labels for plotting
new_data$Pre_Post_Label <- ifelse(new_data$Pre_Post_Effect == -0.5, "Pre", "Post")

new_data <- new_data %>%
  dplyr::select(Event_Index, Pre_Post_Effect, Building_Effect, Agent_Effect, Match,
         ID, Session, predicted_entropy, lower, upper, Pre_Post_Label) %>%
  distinct()  # Remove duplicates if any exist

```




```{r}
# ðŸ“Œ Step 5: Plot the predictions using ggplot2
library(ggplot2)
library(dplyr)

# Define the bins and ensure ordering
new_data <- new_data %>%
  mutate(Event_Bin = factor(case_when(
    Event_Index >= 0 & Event_Index < 3  ~ "0-3",
    Event_Index >= 3 & Event_Index < 10 ~ "3-10",
    Event_Index >= 10 & Event_Index < 20 ~ "10-20",
    Event_Index >= 20 ~ ">20"
  ), levels = c("0-3", "3-10", "10-20", ">20")))  # Ordered bins

# Ensure "Pre" is plotted before "Post"
new_data <- new_data %>%
  mutate(Pre_Post_Label = factor(Pre_Post_Label, levels = c("Pre", "Post")))  # Ordered factor

# Summarize data by bin and Pre_Post_Label with standard error calculation
binned_data <- new_data %>%
  group_by(Event_Bin, Pre_Post_Label) %>%
  summarise(
    mean_entropy = mean(predicted_entropy, na.rm = TRUE),
    se_entropy = sd(predicted_entropy, na.rm = TRUE) / sqrt(n()),
    .groups = 'drop'
  )

# Plot with error bars and value annotations
ggplot(binned_data, aes(x = Event_Bin, y = mean_entropy, fill = Pre_Post_Label)) +
  geom_col(position = position_dodge(width = 0.7), width = 0.6) +  # Bar plot with adjusted width
  geom_errorbar(aes(ymin = mean_entropy - se_entropy, ymax = mean_entropy + se_entropy),
                position = position_dodge(width = 0.7), width = 0.2) +  # Error bars
  geom_text(aes(label = round(mean_entropy, 2)), 
            position = position_dodge(width = 0.7), 
            vjust = -0.5, size = 4) +  # Value annotations
  labs(title = "Binned Effect of number of agent encounters on Entropy",
       subtitle = "Increased entropy after agent encounters in free exploration",
       x = "Number of agent encounters", y = "Mean Predicted Entropy") +
  theme_minimal() +
  scale_fill_manual(values = c("blue", "red"), labels = c("Pre", "Post"))  # Ensuring colors and labels match



```

```{r}

# Save the dataset as a CSV file
write.csv(new_data, "/Volumes/TwoTeras/Resources/new_data.csv", row.names = FALSE)
```

```{r}
ggplot(new_data, aes(x = Event_Index, y = predicted_entropy, color = Pre_Post_Label)) +
  geom_line(size = 1) +  # Smooth line for Pre vs. Post
  labs(title = "Effect of Event Index on Entropy",
       subtitle = "Comparison of Pre and Post Conditions",
       x = "Event Index (Time)", y = "Predicted Entropy") +
  theme_minimal() +
  scale_color_manual(values = c("blue", "red")) +
  scale_fill_manual(values = c("blue", "red"))
```

```{r}
# 1. Filter the dataset to include only the post-fixation windows
df_post <- subset(df_long, Pre_Post_Effect == 0.5)

# 2. Ensure Entropy is numeric and adjusted for Beta modeling
df_post$Entropy <- as.numeric(as.character(df_post$Entropy))

# 3. Cap/shift any edge values so the response stays strictly in (0, 1)
df_post$Entropy[df_post$Entropy >= 1] <- 0.999
df_post$Entropy[df_post$Entropy <= 0] <- 0.001
```

```{r}
contrast_matrix_pairwise <- matrix(
  c(-0.5, -0.5,  0.0,   # Acontextual
     0.5,  0.0, -0.5,   # Congruent
     0.0,  0.5,  0.5),  # Incongruent
  ncol = 3,
  byrow = TRUE
)

rownames(contrast_matrix_pairwise) <- c("Acontextual", "Congruent", "Incongruent")
colnames(contrast_matrix_pairwise) <- c("Congruent_vs_Acontextual", "Incongruent_vs_Acontextual", "Congruent_vs_Incongruent")

contrasts(df_post$Agent_Type) <- contrast_matrix_pairwise


```



```{r}

bayesian_post_model <- brm(
  formula = Entropy ~ Agent_Type + (1 | ID),
  data    = df_post,
  family  = Beta(link = "logit"),
  cores   = parallel::detectCores(),
  chains  = 4,
  iter    = 4000,
  warmup  = 1000,
  control = list(adapt_delta = 0.99),
  save_pars = save_pars(all = TRUE)
)


# 5. Check summary
summary(bayesian_post_model)

```
```{r}
# Load necessary libraries
library(ggplot2)
library(emmeans)
library(tidyverse)

# Extract estimated marginal means (posterior means) for Agent_Type
emmeans_results <- emmeans(bayesian_post_model, ~ Agent_Type)

# Convert to a data frame
plot_data <- as.data.frame(emmeans_results)

# Rename columns for clarity
colnames(plot_data) <- c("Agent_Type", "Mean_Entropy", "SE", "Lower_CI", "Upper_CI")


```


```{r}

library(bayesplot)
library(ggplot2)
library(brms)

# Plot all fixed effects and random effects from the model
mcmc_plot(bayesian_post_model, pars = "^b")  # Plots only fixed effects
```
```{r}

# Load necessary libraries
library(emmeans)
library(tidyverse)
library(brms)

# Extract posterior draws for the model parameters
posterior_draws <- as_draws_df(bayesian_post_model)

# Extract intercept and contrast parameters
posterior_contrasts <- posterior_draws %>%
  select(b_Intercept, b_Agent_TypeCongruent_vs_Acontextual, b_Agent_TypeIncongruent_vs_Acontextual) %>%
  mutate(
    Congruent_vs_Acontextual = b_Intercept + b_Agent_TypeCongruent_vs_Acontextual,
    Incongruent_vs_Acontextual = b_Intercept + b_Agent_TypeIncongruent_vs_Acontextual
  ) %>%
  select(Congruent_vs_Acontextual, Incongruent_vs_Acontextual) %>%
  pivot_longer(cols = everything(), names_to = "Contrast", values_to = "Estimate_Logit")

# Save updated posterior samples for Python
write.csv(posterior_contrasts, "/Volumes/TwoTeras/2_DataSets_Experiments_1_2/Plots/posterior_contrasts_with_intercept.csv", row.names = FALSE)


# Print confirmation
print("Saved posterior contrast samples for Python!")

```
```{r}
# Load necessary libraries
library(emmeans)
library(tidyverse)
library(brms)

# Compute estimated marginal means in logit space
emmeans_results <- emmeans(bayesian_post_model, ~ Agent_Type, type = "link")

# Extract predicted entropy values for Congruent & Incongruent
predicted_entropy <- as_tibble(emmeans_results) %>%
  rename(Estimate_Logit = emmean)  # Rename column for clarity

# Save the predicted entropy values for Python processing
write.csv(predicted_entropy, "/Volumes/TwoTeras/2_DataSets_Experiments_1_2/Plots/predicted_entropy_values.csv", row.names = FALSE)

print("Predicted entropy values saved!")

```
```{r}
# Load necessary libraries
library(tidyverse)
library(brms)

# Create new data for predictions (only Congruent & Incongruent)
new_post_data <- expand.grid(
  Agent_Type = c("Congruent", "Incongruent"),
  ID = "new_ID"  # Dummy ID to avoid random effect issues
)

# Generate posterior predictions
posterior_predictions <- posterior_epred(bayesian_post_model, newdata = new_post_data, re.form = NA)

# Convert posterior predictions into long format
predicted_entropy_samples <- as_tibble(posterior_predictions) %>%
  pivot_longer(cols = everything(), names_to = "Sample", values_to = "Estimate_Logit") %>%
  mutate(Agent_Type = rep(new_post_data$Agent_Type, each = nrow(posterior_predictions)))

# Save all posterior sampled predicted entropy values
write.csv(predicted_entropy_samples, "/Volumes/TwoTeras/2_DataSets_Experiments_1_2/Plots/predicted_entropy_samples.csv", row.names = FALSE)

print("Posterior predicted entropy values saved!")


```


```{r}
# Plot transformed contrasts with credible intervals
ggplot(contrast_data, aes(x = Contrast, y = Response_Estimate, ymin = Lower_Response_CI, ymax = Upper_Response_CI)) +
  geom_point(size = 3, color = "blue") +
  geom_errorbar(width = 0.2) +
  theme_minimal() +
  labs(title = "Posterior Contrasts of Agent Type Effects (Response Scale)",
       x = "Contrast",
       y = "Estimated Change in Entropy (Response Scale)") +
  coord_flip()

```




```{r}
library(brms)

# Bayesian hierarchical model predicting Post_Entropy
bayesian_post_entropy_model <- brm(
  Post_Entropy ~ Building_Effect + Agent_Effect + Match + s(Event_Index, k = 5) + (1 | ID/Session),
  data = df_below_60,
  family = gaussian(),
  cores = parallel::detectCores(),
  chains = 4, iter = 4000, warmup = 1000,
  save_pars = save_pars(all = TRUE)
)

# Model Summary
summary(bayesian_post_entropy_model)

```



```{r}
# Null Model: No predictors, only random effects
bayesian_model_null <- brm(
  Entropy ~ 1 + (1 | ID/Session),
  data = df_long,
  family = Beta(link = "logit"),
  core=4,
  chains = 4, iter = 4000, warmup = 1000,
  save_pars = save_pars(all = TRUE)
)

```

```{r}
library(bridgesampling)

#bridge_full <- bridge_sampler(bayesian_time_model, method = "warp3", repetitions = 50)
bridge_null <- bridge_sampler(bayesian_model_null, method = "warp3", repetitions = 50)

bayes_factor_full_vs_null <- bayes_factor(bridge_full, bridge_null)

print(bayes_factor_full_vs_null)



```
```{r}
library(dplyr)

# Prepare df_post for modeling Post_Entropy only
df_post <- df_combined %>%
  mutate(
    ID = factor(ID),
    Session = factor(Session),
    Condition = factor(Condition, levels = c("Pre", "Post")),
    Agent_Type = factor(Agent_Type),
    Building = factor(Building)
  ) %>%
  arrange(ID, Session, Event_Index)  # Ensure correct chronological order

# Quick check of your dataframe
head(df_post)
```



```{r}
library(BayesFactor)

# Full model (all predictors)
bf_full <- lmBF(Entropy ~ Pre_Post_Effect + Building_Effect + Agent_Effect + Match + Event_Index, 
                data = df_long_clean, 
                whichRandom = c("Participant_ID", "Session"))

# Models removing one term at a time
bf_no_PrePost <- lmBF(Entropy ~ Building_Effect + Agent_Effect + Match + Event_Index, 
                       data = df_long_clean, 
                       whichRandom = c("Participant_ID", "Session"))

bf_no_Building <- lmBF(Entropy ~ Pre_Post_Effect + Agent_Effect + Match + Event_Index, 
                        data = df_long_clean, 
                        whichRandom = c("Participant_ID", "Session"))

bf_no_Agent <- lmBF(Entropy ~ Pre_Post_Effect + Building_Effect + Match + Event_Index, 
                     data = df_long_clean, 
                     whichRandom = c("Participant_ID", "Session"))

bf_no_Match <- lmBF(Entropy ~ Pre_Post_Effect + Building_Effect + Agent_Effect + Event_Index, 
                     data = df_long_clean, 
                     whichRandom = c("Participant_ID", "Session"))

bf_no_EventIndex <- lmBF(Entropy ~ Pre_Post_Effect + Building_Effect + Agent_Effect + Match, 
                          data = df_long_clean, 
                          whichRandom = c("Participant_ID", "Session"))

# Compute individual Bayes Factors
bf_PrePost <- bf_full / bf_no_PrePost
bf_Building <- bf_full / bf_no_Building
bf_Agent <- bf_full / bf_no_Agent
bf_Match <- bf_full / bf_no_Match
bf_EventIndex <- bf_full / bf_no_EventIndex

# Print results
bf_PrePost
bf_Building
bf_Agent
bf_Match
bf_EventIndex

```
```{r}
library(ggplot2)

# Create a dataframe with predictors and their Bayes Factors
bf_data <- data.frame(
  Predictor = c("Pre_Post_Effect", "Building_Effect", "Agent_Effect", "Match", "Event_Index"),
  BayesFactor = c(1.15e+33, 0.0387, 403.70, 0.0387, 2.01e+10)  # Use your computed BF values
)

# Apply log transformation (to make the scale readable)
bf_data$LogBF <- log10(bf_data$BayesFactor)

# Define colors based on importance
bf_data$Category <- ifelse(bf_data$BayesFactor > 30, "Strong Evidence", 
                      ifelse(bf_data$BayesFactor > 3, "Moderate Evidence", 
                      ifelse(bf_data$BayesFactor > 1, "Weak Evidence", "Against Inclusion")))

# Define custom colors
color_palette <- c("Strong Evidence" = "darkred", "Moderate Evidence" = "orange", 
                   "Weak Evidence" = "yellow", "Against Inclusion" = "blue")

# âœ… 2. Plot the Bayes Factors
ggplot(bf_data, aes(x = reorder(Predictor, LogBF), y = LogBF, fill = Category)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = round(BayesFactor, 2)), hjust = 1.2, color = "white", size = 4) +
  scale_fill_manual(values = color_palette) +
  labs(title = "Bayes Factors for Predictors",
       subtitle = "Log-transformed Bayes Factors (BF > 1 favors inclusion, BF < 1 favors exclusion)",
       x = "Predictor",
       y = "Log10(Bayes Factor)") +
  theme_minimal() +
  coord_flip()  # Flips the axes for readability

```




```{r }

# Ensure data is sorted by participant and event order
df <- df %>%
  group_by(Participant_ID) %>%  
  arrange(Gaze_Time) %>%  
  mutate(Event_Index = row_number()) %>%  
  ungroup()

# Function to ensure valid pre-periods
run_causal_impact <- function(df_part) {
  # Convert entropy to a zoo time series
  time_series <- zoo(df_part$Post_Entropy, df_part$Event_Index)
  
  # Set minimum pre-period size
  min_pre_period_size <- max(3, floor(nrow(df_part) / 3))  # At least 3 points
  
  # Ensure post-period starts at a reasonable place
  pre_period_end <- min_pre_period_size
  post_period_start <- pre_period_end + 1
  
  # Define pre/post periods dynamically
  pre_period <- c(1, pre_period_end)
  post_period <- c(post_period_start, nrow(df_part))
  
  # Run CausalImpact only if valid pre-period
  if (length(time_series) > pre_period_end + 2) {
    impact <- CausalImpact(time_series, pre_period, post_period)
    return(impact)
  } else {
    return(NULL)  # Skip small datasets
  }
}

# Run CausalImpact per participant and store results
results_list <- list()
for (participant in unique(df$Participant_ID)) {
  df_part <- df %>% filter(Participant_ID == participant)
  
  impact <- run_causal_impact(df_part)
  
  if (!is.null(impact)) {
    results_list[[participant]] <- summary(impact)
    
    # Save plot
    png(paste0("CausalImpact_", participant, ".png"), width=800, height=600)
    plot(impact)
    dev.off()
  }
}

# Print all participant results
results_list

```
```{r}

```


```{r }
# Run CausalImpact per participant-session
results_list <- list()

for (participant in unique(df$Participant_ID)) {
  df_part <- df %>% filter(Participant_ID == participant)  # Filter participant data
  time_series_part <- zoo(df_part$Post_Entropy, df_part$Event_Index)

  # Adjust pre/post periods for this participant
  pre_period_part <- c(1, floor(nrow(df_part) / 2))
  post_period_part <- c(floor(nrow(df_part) / 2) + 1, nrow(df_part))

  impact <- CausalImpact(time_series_part, pre_period_part, post_period_part)

  # Store results
  results_list[[participant]] <- summary(impact)
  
  # Optional: Save each plot separately
  png(paste0("CausalImpact_", participant, ".png"), width=800, height=600)
  plot(impact)
  dev.off()
}

# Print all participant results
results_list
```
```{r}

```

# ** Condition Model: Pre-Post
To apply CausalImpact, we first convert the entropy values into a time series object.

```{r}
library(lme4)
library(lmerTest)  # For p-values
library(tidyr)
library(dplyr)  # Ensure dplyr is loaded for pipes (%>%)

# Assuming Participant_ID is in the format "1131_1"




# Fit a Linear Mixed Model with random intercepts for participants and sessions
model_nested <- lmer(Entropy ~ Condition + (1 | Participant/Session), data = df_long)

# Print model summary
summary(model_nested)
```
```{r}
library(brms)

bayesian_time_model <- brm(
  Entropy ~ Condition + s(Event_Index) + (1 | Participant/Session),  
  data = df_long,
  family = gaussian(),
  prior = c(
    prior(normal(0, 0.1), class = "b"),  # Prior for fixed effects
    prior(normal(0, 0.1), class = "Intercept"),  # Prior for intercept
    prior(cauchy(0, 1), class = "sd"),  # Random effect priors
    prior(cauchy(0, 1), class = "sds")  # Prior for time smoothing
  ),
  iter = 4000, warmup = 1000, chains = 4, cores = 4,
  control = list(adapt_delta = 0.95)
)

summary(bayesian_time_model)

```

# **Null model
```{r}
library(brms)
library(BayesFactor)  # For direct Bayes Factor computation

# Full Model: Includes Condition (Pre vs. Post)
bayesian_model_full <- brm(
  Entropy ~ Condition + (1 | Participant/Session),  
  data = df_long,
  family = gaussian(),
  prior = c(
    prior(normal(0, 0.1), class = "b"),
    prior(normal(0, 0.1), class = "Intercept"),
    prior(cauchy(0, 1), class = "sd")
  ),
  iter = 4000, warmup = 1000, chains = 4, cores = 4,
  control = list(adapt_delta = 0.95)
)

# Null Model: Excludes Condition
bayesian_model_null <- brm(
  Entropy ~ 1 + (1 | Participant/Session),  # No Condition effect
  data = df_long,
  family = gaussian(),
  prior = c(
    prior(normal(0, 0.1), class = "Intercept"),
    prior(cauchy(0, 1), class = "sd")
  ),
  iter = 4000, warmup = 1000, chains = 4, cores = 4,
  control = list(adapt_delta = 0.95)
)

# Compute Bayes Factor
bayes_factor <- bayes_factor(bayesian_model_full, bayesian_model_null)
print(bayes_factor)

```
# Entropy changes over time AR t-1 
```{r}
bayesian_ar1_model <- brm(
  Entropy ~ Condition + ar(Time_Index, p = 1) + (1 | Participant/Session),  
  data = df_long,
  family = gaussian(),
  prior = c(
    prior(normal(0, 0.1), class = "b"),
    prior(normal(0, 0.1), class = "Intercept"),
    prior(cauchy(0, 1), class = "sd"),
    prior(normal(0, 1), class = "ar")  # Prior for AR(1) effect
  ),
  iter = 4000, warmup = 1000, chains = 4, cores = 4,
  control = list(adapt_delta = 0.95)
)

summary(bayesian_ar1_model)

```
# Gaussian Process Priors (Smooth Entropy Changes Over Time)
If entropy doesnâ€™t change abruptly, we can use Gaussian Process (GP) priors to smooth the effect of time.
```{r}
bayesian_gp_model <- brm(
  Entropy ~ Condition + gp(Time_Index) + (1 | Participant/Session),  
  data = df_long,
  family = gaussian(),
  prior = c(
    prior(normal(0, 0.1), class = "b"),
    prior(normal(0, 0.1), class = "Intercept"),
    prior(cauchy(0, 1), class = "sd"),
    prior(cauchy(0, 1), class = "gp")  # GP prior for time smoothness
  ),
  iter = 4000, warmup = 1000, chains = 4, cores = 4,
  control = list(adapt_delta = 0.95)
)

summary(bayesian_gp_model)
```
# Compare the models 
```{r}
loo_compare(bayesian_model_full, bayesian_ar1_model, bayesian_gp_model)

```

# ** 5. Run CausalImpact Analysis
```{r}
# Run CausalImpact analysis
df$Post_Entropy <- as.numeric(df$Post_Entropy)
time_series <- ts(df$Post_Entropy)

impact <- CausalImpact(time_series, pre_period, post_period)

# Print results
summary(impact)

```
# **6. Visualize the Impact
The following plot displays the predicted vs. actual entropy values, along with the estimated impact of agent gaze.

```{r}
# Save the summary as a text file
writeLines(capture.output(summary(impact)), "CausalImpact_Results.txt")

# Save the plot
png("CausalImpact_Plot.png", width=800, height=600)
plot(impact)
dev.off()

```



