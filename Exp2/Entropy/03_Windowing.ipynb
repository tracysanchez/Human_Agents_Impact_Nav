{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bf01a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import log2\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm  # Import the progress bar library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a18d4a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping dictionary\n",
    "mapping = {\n",
    "    \"TaskBuilding_Public\": \"Task_Building\",\n",
    "    \"TaskBuilding_Residential\": \"Task_Building\",\n",
    "    \"Active_Agent\": \"Agent_Body\",\n",
    "    \"Passive_Agent\": \"Agent_Body\",\n",
    "    \"Active_Agent_Face\": \"Agent_Face\",\n",
    "    \"Passive_Agent_Face\": \"Agent_Face\"\n",
    "}\n",
    "\n",
    "# Collider list\n",
    "collider_list = [\n",
    "    '56_Sa', '39_Sa', '19_Cma', '55_Sa', '25_Cma', '40_Sa', '41_Sa',\n",
    "    '17_Cma', '47_Sa', '03_Cma', '13_Cma', '24_Cma', '01_Cma', '54_Sa',\n",
    "    '15_Cma', '29_Sa', '04_Cma', '49_Sa', '30_Sa', '02_Cma', '51_Sa',\n",
    "    '08_Cma', '28_Cma', '26_Cma', '44_Sa', '06_Cma', '53_Sa', '37_Sa',\n",
    "    '32_Sa', '20_Cma', '16_Cma', '50_Sa', '34_Sa', '11_Cma', '38_Sa',\n",
    "    '33_Sa', '12_Cma', '22_Cma', '42_Sa', '05_Cma', '23_Cma', '18_Cma',\n",
    "    '27_Cma', '45_Sa', '43_Sa', '09_Cma', '31_Sa', '48_Sa', '10_Cma',\n",
    "    '52_Sa', '07_Cma', '46_Sa', '35_Sa', '36_Sa', '21_Cma', '14_Cma'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e602246",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files: 100%|█████████████████████| 145/145 [06:44<00:00,  2.79s/file]\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "input_dir = \"/Volumes/TwoTeras/1_Experiment_2/Eye_Tracking/Pre_processed/05_Debbies_gaze/\"\n",
    "output_trials_dir = \"/Volumes/TwoTeras/1_Experiment_2/Entropy_Results/Window/trials_df/\"\n",
    "output_transition_dir = \"/Volumes/TwoTeras/1_Experiment_2/Entropy_Results/Window/transition_matrix/\"\n",
    "output_entropy_dir = \"/Volumes/TwoTeras/1_Experiment_2/Entropy_Results/Window/entropy_results/\"\n",
    "\n",
    "# Ensure output directories exist\n",
    "os.makedirs(output_trials_dir, exist_ok=True)\n",
    "os.makedirs(output_transition_dir, exist_ok=True)\n",
    "os.makedirs(output_entropy_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# Get list of files to process\n",
    "file_paths = glob.glob(os.path.join(input_dir, \"*.csv\"))\n",
    "\n",
    "# Function to calculate transition entropy\n",
    "def calculate_transition_entropy(matrix, stationary_distribution):\n",
    "    total_entropy = 0\n",
    "    category_entropies = {}\n",
    "    for i, row in matrix.iterrows():\n",
    "        row_entropy = sum(-p * log2(p) for p in row if p > 0)\n",
    "        category_entropies[i] = row_entropy\n",
    "        total_entropy += row_entropy * stationary_distribution.get(i, 0)\n",
    "    return total_entropy, category_entropies\n",
    "\n",
    "# Process all CSV files\n",
    "for file_path in tqdm(file_paths, desc=\"Processing Files\", unit=\"file\"):\n",
    "    # Load the data\n",
    "    data = pd.read_csv(file_path)\n",
    "    data['date_seconds'] = pd.to_datetime(data['timeStampDataPointEnd'], unit='s')\n",
    "\n",
    "    # Filter for the desired gaze events\n",
    "    data_Reduced = data[data['events'] == -2]\n",
    "\n",
    "    # Filter and label rows with colliders\n",
    "    filtered_df = data_Reduced[data_Reduced['names'].isin(collider_list)].copy()\n",
    "    #filtered_df = data_Reduced[data_Reduced['Collider_CategoricalN'].str.contains('TaskBuilding', na=False)].copy()\n",
    "    filtered_df['Occurrence_Order'] = filtered_df.groupby('names').cumcount() + 1\n",
    "\n",
    "    # Segment data by each occurrence of colliders\n",
    "    last_processed_time = {}\n",
    "    trials = []\n",
    "    for index, row in filtered_df.iterrows():\n",
    "        collider_name = row['names']\n",
    "        occurrence_time = row['date_seconds']\n",
    "\n",
    "        # Check if this occurrence falls within the active window\n",
    "        if (\n",
    "            collider_name in last_processed_time\n",
    "            and (occurrence_time - last_processed_time[collider_name]).total_seconds() <= 50\n",
    "        ):\n",
    "            # Skip this occurrence since it's within the 30-second window\n",
    "            continue\n",
    "\n",
    "        # Update the last processed time for this collider\n",
    "        last_processed_time[collider_name] = occurrence_time\n",
    "\n",
    "        # Constrain the 30-second window to the dataset bounds\n",
    "        window_start = max(data_Reduced['date_seconds'].min(), occurrence_time - pd.Timedelta(seconds=5))\n",
    "        window_end = min(data_Reduced['date_seconds'].max(), occurrence_time + pd.Timedelta(seconds=55))\n",
    "\n",
    "        # Extract the constrained window\n",
    "        trial_segment = data_Reduced[\n",
    "            (data_Reduced['date_seconds'] >= window_start) &\n",
    "            (data_Reduced['date_seconds'] <= window_end)\n",
    "        ].copy()\n",
    "\n",
    "        if trial_segment.empty:\n",
    "            continue\n",
    "\n",
    "        # Add trial-specific labels\n",
    "        trial_segment['Collider_Name'] = collider_name\n",
    "        trial_segment['Occurrence_Order'] = len(trials) + 1  # Increment trial count\n",
    "        trial_segment['Trial_ID'] = f\"{collider_name}_Trial_{len(trials) + 1}\"\n",
    "        trials.append(trial_segment)\n",
    "\n",
    "    # Combine all trials into a single DataFrame\n",
    "    if trials:\n",
    "        trials_df = pd.concat(trials, ignore_index=True)\n",
    "    else:\n",
    "        continue  # Skip this file if no trials are found\n",
    "\n",
    "    # Calculate entropy for each trial\n",
    "    entropy_results = []\n",
    "    for trial_id, trial_data in trials_df.groupby('Trial_ID'):\n",
    "        collider_name = trial_data['Collider_Name'].iloc[0]\n",
    "        occurrence_order = trial_data['Occurrence_Order'].iloc[0]\n",
    "\n",
    "        # Build transition matrix\n",
    "        # Apply mapping and extract gaze sequence\n",
    "        trial_data = trial_data.copy()\n",
    "        trial_data[\"Mapped_Column_Collider_Categorical\"] = trial_data[\"Collider_CategoricalN\"].replace(mapping)\n",
    "        gaze_sequence = trial_data['Mapped_Column_Collider_Categorical'].reset_index(drop=True)\n",
    "        categories = gaze_sequence.unique()\n",
    "        transition_matrix = pd.DataFrame(0, index=categories, columns=categories, dtype=float)\n",
    "\n",
    "        for i in range(len(gaze_sequence) - 1):\n",
    "            current_category = gaze_sequence.iloc[i]\n",
    "            next_category = gaze_sequence.iloc[i + 1]\n",
    "            transition_matrix.loc[current_category, next_category] += 1\n",
    "\n",
    "        # Normalize transition matrix\n",
    "        transition_matrix = transition_matrix.div(transition_matrix.sum(axis=1), axis=0).fillna(0)\n",
    "\n",
    "        # Calculate stationary distribution\n",
    "        try:\n",
    "            eigvals, eigvecs = np.linalg.eig(transition_matrix.T)\n",
    "            stationary_distribution = np.real(eigvecs[:, np.isclose(eigvals, 1)].flatten())\n",
    "            stationary_distribution /= stationary_distribution.sum()\n",
    "            stationary_distribution_dict = {categories[i]: stationary_distribution[i] for i in range(len(categories))}\n",
    "        except:\n",
    "            stationary_distribution_dict = {category: 1 / len(categories) for category in categories}\n",
    "\n",
    "        # Calculate entropies\n",
    "        overall_transition_entropy, transition_entropy_per_category = calculate_transition_entropy(\n",
    "            transition_matrix, stationary_distribution_dict\n",
    "        )\n",
    "\n",
    "        # Stationary entropy per category\n",
    "        stationary_entropy_per_category = {\n",
    "            category: (-stationary_distribution_dict[category] * log2(stationary_distribution_dict[category]))\n",
    "            if stationary_distribution_dict[category] > 0 else 0\n",
    "            for category in categories\n",
    "        }\n",
    "\n",
    "        # Normalize entropies\n",
    "        num_categories = len(categories)\n",
    "        normalized_overall_entropy = overall_transition_entropy / log2(num_categories) if num_categories > 1 else 0\n",
    "        normalized_transition_entropy_per_category = {\n",
    "            category: entropy / log2(num_categories) if num_categories > 1 else 0\n",
    "            for category, entropy in transition_entropy_per_category.items()\n",
    "        }\n",
    "        normalized_stationary_entropy_per_category = {\n",
    "            category: entropy / log2(num_categories) if num_categories > 1 else 0\n",
    "            for category, entropy in stationary_entropy_per_category.items()\n",
    "        }\n",
    "\n",
    "        # Store results for this trial\n",
    "        result = {\n",
    "            'Trial_ID': trial_id,\n",
    "            'Collider_Name': collider_name,\n",
    "            'Occurrence_Order': occurrence_order,\n",
    "            'Gaze_Sequence_Length': len(gaze_sequence),\n",
    "            'Overall_Transition_Entropy': normalized_overall_entropy\n",
    "            \n",
    "        }\n",
    "\n",
    "        # Add per-category entropies to the result\n",
    "        for category in categories:\n",
    "            result[f'Transition_Entropy_{category}'] = normalized_transition_entropy_per_category.get(category, 0)\n",
    "            result[f'Stationary_Entropy_{category}'] = normalized_stationary_entropy_per_category.get(category, 0)\n",
    "\n",
    "        entropy_results.append(result)\n",
    "\n",
    "    # Save entropy results for the file\n",
    "    participant_id = file_path[-10:-4]\n",
    "    entropy_df = pd.DataFrame(entropy_results)\n",
    "    entropy_df.to_csv(os.path.join(output_entropy_dir, f\"{participant_id}_entropy_results.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8955121e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
