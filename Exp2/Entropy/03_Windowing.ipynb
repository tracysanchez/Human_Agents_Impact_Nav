{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bf01a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import log2\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm  # Import the progress bar library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e602246",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files: 100%|█████████████████████| 145/145 [05:33<00:00,  2.30s/file]\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "input_dir = \"/Volumes/TwoTeras/1_Experiment_2/Eye_Tracking/Pre_processed/05_Debbies_gaze/\"\n",
    "output_trials_dir = \"/Volumes/TwoTeras/1_Experiment_2/Entropy_Results/Window/trials_df/\"\n",
    "output_transition_dir = \"/Volumes/TwoTeras/1_Experiment_2/Entropy_Results/Window/transition_matrix/\"\n",
    "output_entropy_dir = \"/Volumes/TwoTeras/1_Experiment_2/Entropy_Results/Window/entropy_results/\"\n",
    "\n",
    "# Ensure output directories exist\n",
    "os.makedirs(output_trials_dir, exist_ok=True)\n",
    "os.makedirs(output_transition_dir, exist_ok=True)\n",
    "os.makedirs(output_entropy_dir, exist_ok=True)\n",
    "\n",
    "# Collider list\n",
    "collider_list = [\n",
    "    '56_Sa', '39_Sa', '19_Cma', '55_Sa', '25_Cma', '40_Sa', '41_Sa',\n",
    "    '17_Cma', '47_Sa', '03_Cma', '13_Cma', '24_Cma', '01_Cma', '54_Sa',\n",
    "    '15_Cma', '29_Sa', '04_Cma', '49_Sa', '30_Sa', '02_Cma', '51_Sa',\n",
    "    '08_Cma', '28_Cma', '26_Cma', '44_Sa', '06_Cma', '53_Sa', '37_Sa',\n",
    "    '32_Sa', '20_Cma', '16_Cma', '50_Sa', '34_Sa', '11_Cma', '38_Sa',\n",
    "    '33_Sa', '12_Cma', '22_Cma', '42_Sa', '05_Cma', '23_Cma', '18_Cma',\n",
    "    '27_Cma', '45_Sa', '43_Sa', '09_Cma', '31_Sa', '48_Sa', '10_Cma',\n",
    "    '52_Sa', '07_Cma', '46_Sa', '35_Sa', '36_Sa', '21_Cma', '14_Cma'\n",
    "]\n",
    "\n",
    "# Get list of files to process\n",
    "file_paths = glob.glob(os.path.join(input_dir, \"*.csv\"))\n",
    "\n",
    "# Process all CSV files with a progress bar\n",
    "for file_path in tqdm(file_paths, desc=\"Processing Files\", unit=\"file\"):\n",
    "    # Load the data\n",
    "    data = pd.read_csv(file_path)\n",
    "    data['date_seconds'] = pd.to_datetime(data['timeStampDataPointEnd'], unit='s')\n",
    "\n",
    "    # Filter for the desired gaze events\n",
    "    data_Reduced = data[data['events'] == -2]\n",
    "\n",
    "    # Filter and label rows with colliders\n",
    "    filtered_df = data_Reduced[data_Reduced['names'].isin(collider_list)].copy()\n",
    "    filtered_df['Occurrence_Order'] = filtered_df.groupby('names').cumcount() + 1\n",
    "\n",
    "    # Maintain a dictionary to track the last processed time for each collider\n",
    "    last_processed_time = {}\n",
    "\n",
    "    # Segment data by each occurrence of colliders\n",
    "    trials = []\n",
    "    for index, row in filtered_df.iterrows():\n",
    "        collider_name = row['names']\n",
    "        occurrence_time = row['date_seconds']\n",
    "\n",
    "        # Check if this occurrence falls within the active window\n",
    "        if (\n",
    "            collider_name in last_processed_time\n",
    "            and (occurrence_time - last_processed_time[collider_name]).total_seconds() <= 30\n",
    "        ):\n",
    "            # Skip this occurrence since it's within the 30-second window\n",
    "            continue\n",
    "\n",
    "        # Update the last processed time for this collider\n",
    "        last_processed_time[collider_name] = occurrence_time\n",
    "\n",
    "        # Constrain the 30-second window to the dataset bounds\n",
    "        window_start = max(data_Reduced['date_seconds'].min(), occurrence_time)\n",
    "        window_end = min(data_Reduced['date_seconds'].max(), occurrence_time + pd.Timedelta(seconds=30))\n",
    "\n",
    "        # Extract the constrained window\n",
    "        trial_segment = data_Reduced[\n",
    "            (data_Reduced['date_seconds'] >= window_start) &\n",
    "            (data_Reduced['date_seconds'] <= window_end)\n",
    "        ].copy()\n",
    "\n",
    "        if trial_segment.empty:\n",
    "            continue\n",
    "\n",
    "        # Add trial-specific labels\n",
    "        trial_segment['Collider_Name'] = collider_name\n",
    "        trial_segment['Occurrence_Order'] = len(trials) + 1  # Increment trial count\n",
    "        trial_segment['Trial_ID'] = f\"{collider_name}_Trial_{len(trials) + 1}\"\n",
    "        trials.append(trial_segment)\n",
    "\n",
    "    # Combine all trials into a single DataFrame\n",
    "    if trials:\n",
    "        trials_df = pd.concat(trials, ignore_index=True)\n",
    "    else:\n",
    "        continue  # Skip this file if no trials are found\n",
    "\n",
    "    # Save trials_df\n",
    "    participant_id = file_path[-10:-4]\n",
    "    trials_df.to_csv(os.path.join(output_trials_dir, f\"{participant_id}_trials_df.csv\"), index=False)\n",
    "\n",
    "    # Calculate transition matrices and entropy\n",
    "    entropy_results = []\n",
    "    for trial_id, trial_data in trials_df.groupby('Trial_ID'):\n",
    "        collider_name = trial_data['Collider_Name'].iloc[0]\n",
    "        occurrence_order = trial_data['Occurrence_Order'].iloc[0]\n",
    "\n",
    "        gaze_sequence = trial_data['Collider_CategoricalN'].reset_index(drop=True)\n",
    "        categories = gaze_sequence.unique()\n",
    "        transition_matrix = pd.DataFrame(0, index=categories, columns=categories, dtype=float)\n",
    "\n",
    "        # Build the transition matrix\n",
    "        for i in range(len(gaze_sequence) - 1):\n",
    "            current_category = gaze_sequence.iloc[i]\n",
    "            next_category = gaze_sequence.iloc[i + 1]\n",
    "            transition_matrix.loc[current_category, next_category] += 1\n",
    "\n",
    "        # Normalize the transition matrix\n",
    "        transition_matrix = transition_matrix.div(transition_matrix.sum(axis=1), axis=0).fillna(0)\n",
    "\n",
    "        # Save transition matrix\n",
    "        transition_matrix.to_csv(os.path.join(output_transition_dir, f\"{participant_id}_transition_matrix.csv\"))\n",
    "\n",
    "        # Calculate stationary distribution\n",
    "        try:\n",
    "            eigvals, eigvecs = np.linalg.eig(transition_matrix.T)\n",
    "            stationary_distribution = np.real(eigvecs[:, np.isclose(eigvals, 1)].flatten())\n",
    "            stationary_distribution /= stationary_distribution.sum()\n",
    "\n",
    "            stationary_distribution_dict = {categories[i]: stationary_distribution[i] for i in range(len(categories))}\n",
    "        except:\n",
    "            stationary_distribution_dict = {category: 1 / len(categories) for category in categories}\n",
    "\n",
    "        # Calculate entropy\n",
    "        def calculate_transition_entropy(matrix, stationary_distribution):\n",
    "            total_entropy = 0\n",
    "            for i, row in matrix.iterrows():\n",
    "                row_entropy = sum(-p * log2(p) for p in row if p > 0)\n",
    "                total_entropy += row_entropy * stationary_distribution.get(i, 0)\n",
    "            return total_entropy\n",
    "\n",
    "        overall_transition_entropy = calculate_transition_entropy(transition_matrix, stationary_distribution_dict)\n",
    "\n",
    "        num_categories = len(transition_matrix)\n",
    "        normalized_overall_entropy = overall_transition_entropy / log2(num_categories) if num_categories > 1 else 0\n",
    "\n",
    "        result = {\n",
    "            'Trial_ID': trial_id,\n",
    "            'Collider_Name': collider_name,\n",
    "            'Occurrence_Order': occurrence_order,\n",
    "            'Overall_Transition_Entropy': normalized_overall_entropy\n",
    "        }\n",
    "        entropy_results.append(result)\n",
    "\n",
    "    # Save entropy results\n",
    "    entropy_df = pd.DataFrame(entropy_results)\n",
    "    entropy_df.to_csv(os.path.join(output_entropy_dir, f\"{participant_id}_entropy_results.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8955121e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
