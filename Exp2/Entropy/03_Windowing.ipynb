{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bf01a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import log2\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm  # Import the progress bar library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a18d4a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping dictionary\n",
    "mapping = {\n",
    "    \"TaskBuilding_Public\": \"Task_Building\",\n",
    "    \"TaskBuilding_Residential\": \"Task_Building\",\n",
    "    \"Active_Agent\": \"Agent\",\n",
    "    \"Passive_Agent\": \"Agent\",\n",
    "    \"Active_Agent_Face\": \"Agent\",\n",
    "    \"Passive_Agent_Face\": \"Agent\"\n",
    "}\n",
    "\n",
    "# Collider list\n",
    "collider_list = [\n",
    "    '56_Sa', '39_Sa', '19_Cma', '55_Sa', '25_Cma', '40_Sa', '41_Sa',\n",
    "    '17_Cma', '47_Sa', '03_Cma', '13_Cma', '24_Cma', '01_Cma', '54_Sa',\n",
    "    '15_Cma', '29_Sa', '04_Cma', '49_Sa', '30_Sa', '02_Cma', '51_Sa',\n",
    "    '08_Cma', '28_Cma', '26_Cma', '44_Sa', '06_Cma', '53_Sa', '37_Sa',\n",
    "    '32_Sa', '20_Cma', '16_Cma', '50_Sa', '34_Sa', '11_Cma', '38_Sa',\n",
    "    '33_Sa', '12_Cma', '22_Cma', '42_Sa', '05_Cma', '23_Cma', '18_Cma',\n",
    "    '27_Cma', '45_Sa', '43_Sa', '09_Cma', '31_Sa', '48_Sa', '10_Cma',\n",
    "    '52_Sa', '07_Cma', '46_Sa', '35_Sa', '36_Sa', '21_Cma', '14_Cma'\n",
    "]\n",
    "\n",
    "# Define paths\n",
    "input_dir = \"/Volumes/TwoTeras/1_Experiment_2/Eye_Tracking/Pre_processed/05_Debbies_gaze/\"\n",
    "output_trials_dir = \"/Volumes/TwoTeras/1_Experiment_2/Entropy_Results/Window/trials_df/\"\n",
    "output_transition_dir = \"/Volumes/TwoTeras/1_Experiment_2/Entropy_Results/Window/transition_matrix/\"\n",
    "output_entropy_dir = \"/Volumes/TwoTeras/1_Experiment_2/Entropy_Results/Window/entropy_results/Chao_Shen/\"\n",
    "\n",
    "# Ensure output directories exist\n",
    "os.makedirs(output_trials_dir, exist_ok=True)\n",
    "os.makedirs(output_transition_dir, exist_ok=True)\n",
    "os.makedirs(output_entropy_dir, exist_ok=True)\n",
    "\n",
    "file_paths = glob.glob(os.path.join(input_dir, \"*.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f802a4c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a50c37c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate Chao-Shen entropy\n",
    "def chao_shen(q):\n",
    "    yx = q[q > 0]  # Remove bins with zero counts\n",
    "    n = np.sum(yx)  # Total count\n",
    "    p = yx.astype(float) / n  # Observed probabilities\n",
    "    f1 = np.sum(yx == 1)  # Number of singletons in the sample\n",
    "\n",
    "    if f1 == n:  # Avoid division by zero when all are singletons\n",
    "        f1 -= 1\n",
    "\n",
    "    C = 1 - (f1 / n)  # Estimated sample coverage\n",
    "    pa = C * p  # Coverage-adjusted probabilities\n",
    "    la = 1 - (1 - pa) ** n  # Probability of observing each category\n",
    "    H = -np.sum((pa * np.log2(pa)) / la)  # Chao-Shen entropy\n",
    "\n",
    "    return H, pa, la\n",
    "\n",
    "# Function to calculate Chao-Shen transition entropy\n",
    "def calculate_chao_shen_transition_entropy(raw_matrix, stationary_distribution):\n",
    "    \"\"\"\n",
    "    Calculate entropy with Chao-Shen correction using the raw transition matrix counts.\n",
    "    \"\"\"\n",
    "    chao_shen_total_entropy = 0\n",
    "    chao_shen_category_entropies = {}\n",
    "\n",
    "    for i, row in raw_matrix.iterrows():\n",
    "        counts = row.values.astype(int)  # Use raw counts for Chao-Shen\n",
    "        if (counts.sum() > 1) & (len(row[row == 0.0]) < (len(row) - 1)):\n",
    "            H, _, _ = chao_shen(counts)  # Apply Chao-Shen correction\n",
    "            chao_shen_category_entropies[i] = H\n",
    "            chao_shen_total_entropy += H * stationary_distribution.get(i, 0)  # Weighted by stationary distribution\n",
    "    return chao_shen_total_entropy, chao_shen_category_entropies\n",
    "\n",
    "\n",
    "# Function to calculate transition entropy\n",
    "def calculate_transition_entropy(matrix, stationary_distribution):\n",
    "    total_entropy = np.NaN\n",
    "    category_entropies = {}\n",
    "    for i, row in matrix.iterrows():\n",
    "        row_entropy = sum(-p * log2(p) for p in row if p > 0)\n",
    "        category_entropies[i] = row_entropy\n",
    "        total_entropy += row_entropy * stationary_distribution.get(i, 0)\n",
    "    return total_entropy, category_entropies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef36c2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files:   1%|▏                      | 1/145 [00:02<05:14,  2.18s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Stationary distribution does not sum to 1 (sum=0.0). Normalizing.\n",
      "Warning: Using fallback stationary distribution due to error: index 0 is out of bounds for axis 0 with size 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files:   6%|█▍                     | 9/145 [00:22<05:48,  2.56s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Stationary distribution does not sum to 1 (sum=0.0). Normalizing.\n",
      "Warning: Using fallback stationary distribution due to error: index 0 is out of bounds for axis 0 with size 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files:   9%|█▉                    | 13/145 [00:32<05:33,  2.53s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Stationary distribution does not sum to 1 (sum=0.0). Normalizing.\n",
      "Warning: Using fallback stationary distribution due to error: index 0 is out of bounds for axis 0 with size 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files:  26%|█████▌                | 37/145 [01:22<03:30,  1.95s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Stationary distribution does not sum to 1 (sum=0.0). Normalizing.\n",
      "Warning: Using fallback stationary distribution due to error: index 0 is out of bounds for axis 0 with size 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Files:  26%|█████▊                | 38/145 [01:25<03:48,  2.13s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Stationary distribution does not sum to 1 (sum=0.0). Normalizing.\n",
      "Warning: Using fallback stationary distribution due to error: index 0 is out of bounds for axis 0 with size 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing Files:  27%|█████▉                | 39/145 [01:27<03:48,  2.15s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Stationary distribution does not sum to 1 (sum=0.0). Normalizing.\n",
      "Warning: Using fallback stationary distribution due to error: index 0 is out of bounds for axis 0 with size 0\n",
      "Warning: Stationary distribution does not sum to 1 (sum=0.0). Normalizing.\n",
      "Warning: Using fallback stationary distribution due to error: index 0 is out of bounds for axis 0 with size 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files:  37%|████████▏             | 54/145 [01:59<03:09,  2.08s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Stationary distribution does not sum to 1 (sum=0.0). Normalizing.\n",
      "Warning: Using fallback stationary distribution due to error: index 0 is out of bounds for axis 0 with size 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files:  44%|█████████▋            | 64/145 [02:24<03:09,  2.33s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Stationary distribution does not sum to 1 (sum=0.0). Normalizing.\n",
      "Warning: Using fallback stationary distribution due to error: index 0 is out of bounds for axis 0 with size 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files:  46%|██████████            | 66/145 [02:29<03:08,  2.39s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Stationary distribution does not sum to 1 (sum=0.0). Normalizing.\n",
      "Warning: Using fallback stationary distribution due to error: index 0 is out of bounds for axis 0 with size 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files:  48%|██████████▌           | 70/145 [02:39<03:06,  2.48s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Stationary distribution does not sum to 1 (sum=0.0). Normalizing.\n",
      "Warning: Using fallback stationary distribution due to error: index 0 is out of bounds for axis 0 with size 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files:  50%|██████████▉           | 72/145 [02:44<02:50,  2.34s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Stationary distribution does not sum to 1 (sum=0.0). Normalizing.\n",
      "Warning: Using fallback stationary distribution due to error: index 0 is out of bounds for axis 0 with size 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files:  51%|███████████▏          | 74/145 [02:49<02:55,  2.48s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Stationary distribution does not sum to 1 (sum=0.0). Normalizing.\n",
      "Warning: Using fallback stationary distribution due to error: index 0 is out of bounds for axis 0 with size 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files:  57%|████████████▍         | 82/145 [03:08<02:30,  2.38s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Stationary distribution does not sum to 1 (sum=0.0). Normalizing.\n",
      "Warning: Using fallback stationary distribution due to error: index 0 is out of bounds for axis 0 with size 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files:  58%|████████████▋         | 84/145 [03:13<02:29,  2.46s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Stationary distribution does not sum to 1 (sum=0.0). Normalizing.\n",
      "Warning: Using fallback stationary distribution due to error: index 0 is out of bounds for axis 0 with size 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files:  59%|█████████████         | 86/145 [03:17<02:21,  2.40s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Stationary distribution does not sum to 1 (sum=0.0). Normalizing.\n",
      "Warning: Using fallback stationary distribution due to error: index 0 is out of bounds for axis 0 with size 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files:  65%|██████████████▎       | 94/145 [03:38<01:54,  2.24s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Stationary distribution does not sum to 1 (sum=0.0). Normalizing.\n",
      "Warning: Using fallback stationary distribution due to error: index 0 is out of bounds for axis 0 with size 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files:  80%|████████████████▊    | 116/145 [04:33<01:11,  2.47s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Stationary distribution does not sum to 1 (sum=0.0). Normalizing.\n",
      "Warning: Using fallback stationary distribution due to error: index 0 is out of bounds for axis 0 with size 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files:  91%|███████████████████  | 132/145 [05:11<00:30,  2.34s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Stationary distribution does not sum to 1 (sum=0.0). Normalizing.\n",
      "Warning: Using fallback stationary distribution due to error: index 0 is out of bounds for axis 0 with size 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files:  95%|███████████████████▉ | 138/145 [05:22<00:14,  2.11s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Stationary distribution does not sum to 1 (sum=0.0). Normalizing.\n",
      "Warning: Using fallback stationary distribution due to error: index 0 is out of bounds for axis 0 with size 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files:  99%|████████████████████▋| 143/145 [05:36<00:05,  2.61s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Stationary distribution does not sum to 1 (sum=0.0). Normalizing.\n",
      "Warning: Using fallback stationary distribution due to error: index 0 is out of bounds for axis 0 with size 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files: 100%|█████████████████████| 145/145 [05:41<00:00,  2.35s/file]\n"
     ]
    }
   ],
   "source": [
    "# Process all CSV files\n",
    "for file_path in tqdm(file_paths, desc=\"Processing Files\", unit=\"file\"):\n",
    "    # Extract participant ID from the file path\n",
    "    participant_id = os.path.basename(file_path).split('.')[0]\n",
    "\n",
    "    # Load the data\n",
    "    data = pd.read_csv(file_path)\n",
    "    data['date_seconds'] = pd.to_datetime(data['timeStampDataPointEnd'], unit='s')\n",
    "\n",
    "    # Filter for the desired gaze events\n",
    "    data_Reduced = data[data['events'] == -2]\n",
    "\n",
    "    # Filter and label rows with colliders\n",
    "    filtered_df = data_Reduced[data_Reduced['names'].isin(collider_list)].copy()\n",
    "    filtered_df['Occurrence_Order'] = filtered_df.groupby('names').cumcount() + 1\n",
    "\n",
    "    # Segment data by each occurrence of colliders\n",
    "    last_processed_time = {}\n",
    "    trials = []\n",
    "    trial_window_times = {}  # Dictionary to store window times for each trial\n",
    "    for index, row in filtered_df.iterrows():\n",
    "        collider_name = row['names']\n",
    "        occurrence_time = row['date_seconds']\n",
    "\n",
    "        if (\n",
    "            collider_name in last_processed_time \n",
    "            and (occurrence_time - last_processed_time[collider_name]).total_seconds() <=30\n",
    "        ):\n",
    "            continue\n",
    "\n",
    "        last_processed_time[collider_name] = occurrence_time\n",
    "\n",
    "        window_start = max(data_Reduced['date_seconds'].min(), occurrence_time)\n",
    "        window_end = min(data_Reduced['date_seconds'].max(), occurrence_time + pd.Timedelta(seconds=30))\n",
    "\n",
    "        trial_segment = data_Reduced[\n",
    "            (data_Reduced['date_seconds'] >= window_start) &\n",
    "            (data_Reduced['date_seconds'] <= window_end)\n",
    "        ].copy()\n",
    "        trial_id = f\"{collider_name}_Trial_{len(trials) + 1}\"\n",
    "\n",
    "        trial_segment['Collider_Name'] = collider_name\n",
    "        trial_segment['Occurrence_Order'] = len(trials) + 1\n",
    "        trial_segment['Trial_ID'] = trial_id\n",
    "        \n",
    "        trials.append(trial_segment)\n",
    "        \n",
    "        # Store the trial's window start and end times\n",
    "        trial_window_times[trial_id] = (window_start, window_end)\n",
    "\n",
    "    if trials:\n",
    "        trials_df = pd.concat(trials, ignore_index=True)\n",
    "        trials_df.to_csv(os.path.join(output_trials_dir, f\"{os.path.basename(file_path).replace('.csv', '_trials.csv')}\"), index=False)\n",
    "    else:\n",
    "        print(f\"No trials found for file: {file_path}\")\n",
    "        continue\n",
    "\n",
    "    # Initialize a list to store entropy results for this participant\n",
    "    chao_shen_entropy_results = []\n",
    "\n",
    "    # Calculate entropy for each trial\n",
    "    for trial_id, trial_data in trials_df.groupby('Trial_ID'):\n",
    "        collider_name = trial_data['Collider_Name'].iloc[0]\n",
    "        occurrence_order = trial_data['Occurrence_Order'].iloc[0]\n",
    "\n",
    "        trial_data[\"Mapped_Column_Collider_Categorical\"] = trial_data[\"Collider_CategoricalN\"].replace(mapping)\n",
    "        gaze_sequence = trial_data['Mapped_Column_Collider_Categorical'].reset_index(drop=True)\n",
    "\n",
    "        categories = gaze_sequence.unique()\n",
    "        raw_transition_matrix = pd.DataFrame(0, index=categories, columns=categories, dtype=float)\n",
    "\n",
    "        # Build raw transition matrix\n",
    "        for i in range(len(gaze_sequence) - 1):\n",
    "            current_category = gaze_sequence.iloc[i]\n",
    "            next_category = gaze_sequence.iloc[i + 1]\n",
    "            raw_transition_matrix.loc[current_category, next_category] += 1\n",
    "\n",
    "        # Normalize transition matrix rows\n",
    "        transition_matrix = raw_transition_matrix.div(raw_transition_matrix.sum(axis=1), axis=0).fillna(0)\n",
    "\n",
    "        # Ensure proper normalization of the stationary distribution\n",
    "        try:\n",
    "            eigvals, eigvecs = np.linalg.eig(transition_matrix.T)\n",
    "            stationary_distribution = np.real(eigvecs[:, np.isclose(eigvals, 1)].flatten())\n",
    "            stationary_distribution /= stationary_distribution.sum()\n",
    "\n",
    "            if not np.isclose(stationary_distribution.sum(), 1):\n",
    "                print(f\"Warning: Stationary distribution does not sum to 1 (sum={stationary_distribution.sum()}). Normalizing.\")\n",
    "                stationary_distribution /= stationary_distribution.sum()\n",
    "\n",
    "            stationary_distribution_dict = {categories[i]: stationary_distribution[i] for i in range(len(categories))}\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Using fallback stationary distribution due to error: {e}\")\n",
    "            stationary_distribution_dict = {category: 1 / len(categories) for category in categories}\n",
    "\n",
    "        # Calculate Chao-Shen transition entropy\n",
    "        chao_shen_overall_transition_entropy, chao_shen_transition_entropy_per_category = calculate_chao_shen_transition_entropy(\n",
    "            raw_transition_matrix, stationary_distribution_dict\n",
    "        )\n",
    "\n",
    "        # Normalize entropies\n",
    "        num_categories = len(categories)\n",
    "        normalized_chao_shen_overall_entropy = (\n",
    "            chao_shen_overall_transition_entropy / np.log2(num_categories) if num_categories > 1 else 0\n",
    "        )\n",
    "        normalized_chao_shen_transition_entropy_per_category = {\n",
    "            category: entropy / np.log2(num_categories) if num_categories > 1 else 0\n",
    "            for category, entropy in chao_shen_transition_entropy_per_category.items()\n",
    "        }\n",
    "\n",
    "        # Store results for this trial\n",
    "        chao_shen_result = {\n",
    "            'Trial_ID': trial_id,\n",
    "            'Collider_Name': collider_name,\n",
    "            'Occurrence_Order': occurrence_order,\n",
    "            'Gaze_Sequence_Length': len(gaze_sequence),\n",
    "            'Chao_Shen_Overall_Transition_Entropy': normalized_chao_shen_overall_entropy,\n",
    "            'Window_Start': window_start,  \n",
    "            'Window_End': window_end,     \n",
    "        }\n",
    "\n",
    "        # Add per-category entropies to the result\n",
    "        for category in categories:\n",
    "            chao_shen_result[f'Chao_Shen_Transition_Entropy_{category}'] = normalized_chao_shen_transition_entropy_per_category.get(\n",
    "                category, 0\n",
    "            )\n",
    "        chao_shen_entropy_results.append(chao_shen_result)\n",
    "\n",
    "    # Save all results for this participant to a single file\n",
    "    chao_shen_entropy_df = pd.DataFrame(chao_shen_entropy_results)\n",
    "    chao_shen_entropy_df.to_csv(os.path.join(output_entropy_dir, f\"Chao_Shen_{participant_id}_entropy_results.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015ef278",
   "metadata": {},
   "source": [
    "# Get list of files to process\n",
    "file_paths = glob.glob(os.path.join(input_dir, \"*.csv\"))\n",
    "\n",
    "# Function to calculate transition entropy\n",
    "def calculate_transition_entropy(matrix, stationary_distribution):\n",
    "    total_entropy = 0\n",
    "    category_entropies = {}\n",
    "    for i, row in matrix.iterrows():\n",
    "        row_entropy = sum(-p * log2(p) for p in row if p > 0)\n",
    "        category_entropies[i] = row_entropy\n",
    "        total_entropy += row_entropy * stationary_distribution.get(i, 0)\n",
    "    return total_entropy, category_entropies\n",
    "\n",
    "# Process all CSV files\n",
    "for file_path in tqdm(file_paths, desc=\"Processing Files\", unit=\"file\"):\n",
    "    # Load the data\n",
    "    data = pd.read_csv(file_path)\n",
    "    data['date_seconds'] = pd.to_datetime(data['timeStampDataPointEnd'], unit='s')\n",
    "\n",
    "    # Filter for the desired gaze events\n",
    "    data_Reduced = data[data['events'] == -2]\n",
    "\n",
    "    # Filter and label rows with colliders\n",
    "    filtered_df = data_Reduced[data_Reduced['names'].isin(collider_list)].copy()\n",
    "    #filtered_df = data_Reduced[data_Reduced['Collider_CategoricalN'].str.contains('TaskBuilding', na=False)].copy()\n",
    "    filtered_df['Occurrence_Order'] = filtered_df.groupby('names').cumcount() + 1\n",
    "\n",
    "    # Segment data by each occurrence of colliders\n",
    "    last_processed_time = {}\n",
    "    trials = []\n",
    "    for index, row in filtered_df.iterrows():\n",
    "        collider_name = row['names']\n",
    "        occurrence_time = row['date_seconds']\n",
    "\n",
    "        # Check if this occurrence falls within the active window\n",
    "        if (\n",
    "            collider_name in last_processed_time\n",
    "            and (occurrence_time - last_processed_time[collider_name]).total_seconds() <= 30\n",
    "        ):\n",
    "            # Skip this occurrence since it's within the 30-second window\n",
    "            continue\n",
    "\n",
    "        # Update the last processed time for this collider\n",
    "        last_processed_time[collider_name] = occurrence_time\n",
    "\n",
    "        # Constrain the 30-second window to the dataset bounds\n",
    "        window_start = max(data_Reduced['date_seconds'].min(), occurrence_time - pd.Timedelta(seconds=5))\n",
    "        window_end = min(data_Reduced['date_seconds'].max(), occurrence_time + pd.Timedelta(seconds=25))\n",
    "\n",
    "        # Extract the constrained window\n",
    "        trial_segment = data_Reduced[\n",
    "            (data_Reduced['date_seconds'] >= window_start) &\n",
    "            (data_Reduced['date_seconds'] <= window_end)\n",
    "        ].copy()\n",
    "\n",
    "        if trial_segment.empty:\n",
    "            continue\n",
    "\n",
    "        # Add trial-specific labels\n",
    "        trial_segment['Collider_Name'] = collider_name\n",
    "        trial_segment['Occurrence_Order'] = len(trials) + 1  # Increment trial count\n",
    "        trial_segment['Trial_ID'] = f\"{collider_name}_Trial_{len(trials) + 1}\"\n",
    "        trials.append(trial_segment)\n",
    "\n",
    "    # Combine all trials into a single DataFrame\n",
    "    if trials:\n",
    "        trials_df = pd.concat(trials, ignore_index=True)\n",
    "    else:\n",
    "        continue  # Skip this file if no trials are found\n",
    "\n",
    "    # Calculate entropy for each trial\n",
    "    entropy_results = []\n",
    "    for trial_id, trial_data in trials_df.groupby('Trial_ID'):\n",
    "        collider_name = trial_data['Collider_Name'].iloc[0]\n",
    "        occurrence_order = trial_data['Occurrence_Order'].iloc[0]\n",
    "\n",
    "        # Build transition matrix\n",
    "        # Apply mapping and extract gaze sequence\n",
    "        trial_data = trial_data.copy()\n",
    "        trial_data[\"Mapped_Column_Collider_Categorical\"] = trial_data[\"Collider_CategoricalN\"].replace(mapping)\n",
    "        gaze_sequence = trial_data['Mapped_Column_Collider_Categorical'].reset_index(drop=True)\n",
    "        categories = gaze_sequence.unique()\n",
    "        transition_matrix = pd.DataFrame(0, index=categories, columns=categories, dtype=float)\n",
    "\n",
    "        for i in range(len(gaze_sequence) - 1):\n",
    "            current_category = gaze_sequence.iloc[i]\n",
    "            next_category = gaze_sequence.iloc[i + 1]\n",
    "            transition_matrix.loc[current_category, next_category] += 1\n",
    "\n",
    "        \n",
    "        # Normalize transition matrix\n",
    "        transition_matrix = transition_matrix.div(transition_matrix.sum(axis=1), axis=0).fillna(0)\n",
    "\n",
    "        # Calculate stationary distribution\n",
    "        try:\n",
    "            eigvals, eigvecs = np.linalg.eig(transition_matrix.T)\n",
    "            stationary_distribution = np.real(eigvecs[:, np.isclose(eigvals, 1)].flatten())\n",
    "            stationary_distribution /= stationary_distribution.sum()\n",
    "            stationary_distribution_dict = {categories[i]: stationary_distribution[i] for i in range(len(categories))}\n",
    "        except:\n",
    "            stationary_distribution_dict = {category: 1 / len(categories) for category in categories}\n",
    "\n",
    "        # Calculate entropies\n",
    "        overall_transition_entropy, transition_entropy_per_category = calculate_transition_entropy(\n",
    "            transition_matrix, stationary_distribution_dict\n",
    "        )\n",
    "\n",
    "        # Stationary entropy per category\n",
    "        stationary_entropy_per_category = {\n",
    "            category: (-stationary_distribution_dict[category] * log2(stationary_distribution_dict[category]))\n",
    "            if stationary_distribution_dict[category] > 0 else 0\n",
    "            for category in categories\n",
    "        }\n",
    "\n",
    "        # Normalize entropies\n",
    "        num_categories = len(categories)\n",
    "        normalized_overall_entropy = overall_transition_entropy / log2(num_categories) if num_categories > 1 else 0\n",
    "        normalized_transition_entropy_per_category = {\n",
    "            category: entropy / log2(num_categories) if num_categories > 1 else 0\n",
    "            for category, entropy in transition_entropy_per_category.items()\n",
    "        }\n",
    "        normalized_stationary_entropy_per_category = {\n",
    "            category: entropy / log2(num_categories) if num_categories > 1 else 0\n",
    "            for category, entropy in stationary_entropy_per_category.items()\n",
    "        }\n",
    "\n",
    "        # Store results for this trial\n",
    "        result = {\n",
    "            'Trial_ID': trial_id,\n",
    "            'Collider_Name': collider_name,\n",
    "            'Occurrence_Order': occurrence_order,\n",
    "            'Gaze_Sequence_Length': len(gaze_sequence),\n",
    "            'Overall_Transition_Entropy': normalized_overall_entropy\n",
    "            \n",
    "        }\n",
    "\n",
    "        # Add per-category entropies to the result\n",
    "        for category in categories:\n",
    "            result[f'Transition_Entropy_{category}'] = normalized_transition_entropy_per_category.get(category, 0)\n",
    "            result[f'Stationary_Entropy_{category}'] = normalized_stationary_entropy_per_category.get(category, 0)\n",
    "\n",
    "        entropy_results.append(result)\n",
    "\n",
    "    # Save entropy results for the file\n",
    "    participant_id = file_path[-10:-4]\n",
    "    entropy_df = pd.DataFrame(entropy_results)\n",
    "    entropy_df.to_csv(os.path.join(output_entropy_dir, f\"{participant_id}_entropy_results.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c55da59",
   "metadata": {},
   "source": [
    "# Define paths\n",
    "input_dir = \"/Volumes/TwoTeras/1_Experiment_2/Eye_Tracking/Pre_processed/05_Debbies_gaze/\"\n",
    "output_trials_dir = \"/Volumes/TwoTeras/1_Experiment_2/Entropy_Results/Window/trials_df/\"\n",
    "output_transition_dir = \"/Volumes/TwoTeras/1_Experiment_2/Entropy_Results/Window/transition_matrix/\"\n",
    "output_entropy_dir = \"/Volumes/TwoTeras/1_Experiment_2/Entropy_Results/Window/entropy_results/\"\n",
    "\n",
    "# Ensure output directories exist\n",
    "os.makedirs(output_trials_dir, exist_ok=True)\n",
    "os.makedirs(output_transition_dir, exist_ok=True)\n",
    "os.makedirs(output_entropy_dir, exist_ok=True)\n",
    "\n",
    "# Collider list\n",
    "collider_list = [\n",
    "    '56_Sa', '39_Sa', '19_Cma', '55_Sa', '25_Cma', '40_Sa', '41_Sa',\n",
    "    '17_Cma', '47_Sa', '03_Cma', '13_Cma', '24_Cma', '01_Cma', '54_Sa',\n",
    "    '15_Cma', '29_Sa', '04_Cma', '49_Sa', '30_Sa', '02_Cma', '51_Sa',\n",
    "    '08_Cma', '28_Cma', '26_Cma', '44_Sa', '06_Cma', '53_Sa', '37_Sa',\n",
    "    '32_Sa', '20_Cma', '16_Cma', '50_Sa', '34_Sa', '11_Cma', '38_Sa',\n",
    "    '33_Sa', '12_Cma', '22_Cma', '42_Sa', '05_Cma', '23_Cma', '18_Cma',\n",
    "    '27_Cma', '45_Sa', '43_Sa', '09_Cma', '31_Sa', '48_Sa', '10_Cma',\n",
    "    '52_Sa', '07_Cma', '46_Sa', '35_Sa', '36_Sa', '21_Cma', '14_Cma'\n",
    "]\n",
    "\n",
    "# Get list of files to process\n",
    "file_paths = glob.glob(os.path.join(input_dir, \"*.csv\"))\n",
    "\n",
    "# Process all CSV files with a progress bar\n",
    "for file_path in tqdm(file_paths, desc=\"Processing Files\", unit=\"file\"):\n",
    "    # Load the data\n",
    "    data = pd.read_csv(file_path)\n",
    "    data['date_seconds'] = pd.to_datetime(data['timeStampDataPointEnd'], unit='s')\n",
    "\n",
    "    # Filter for the desired gaze events\n",
    "    data_Reduced = data[data['events'] == -2]\n",
    "\n",
    "    # Filter and label rows with colliders\n",
    "    filtered_df = data_Reduced[data_Reduced['names'].isin(collider_list)].copy()\n",
    "    filtered_df['Occurrence_Order'] = filtered_df.groupby('names').cumcount() + 1\n",
    "\n",
    "    # Maintain a dictionary to track the last processed time for each collider\n",
    "    last_processed_time = {}\n",
    "\n",
    "    # Segment data by each occurrence of colliders\n",
    "    trials = []\n",
    "    for index, row in filtered_df.iterrows():\n",
    "        collider_name = row['names']\n",
    "        occurrence_time = row['date_seconds']\n",
    "\n",
    "        # Check if this occurrence falls within the active window\n",
    "        if (\n",
    "            collider_name in last_processed_time\n",
    "            and (occurrence_time - last_processed_time[collider_name]).total_seconds() <= 30\n",
    "        ):\n",
    "            # Skip this occurrence since it's within the 30-second window\n",
    "            continue\n",
    "\n",
    "        # Update the last processed time for this collider\n",
    "        last_processed_time[collider_name] = occurrence_time\n",
    "\n",
    "        # Constrain the 30-second window to the dataset bounds\n",
    "        window_start = max(data_Reduced['date_seconds'].min(), occurrence_time)\n",
    "        window_end = min(data_Reduced['date_seconds'].max(), occurrence_time + pd.Timedelta(seconds=30))\n",
    "\n",
    "        # Extract the constrained window\n",
    "        trial_segment = data_Reduced[\n",
    "            (data_Reduced['date_seconds'] >= window_start) &\n",
    "            (data_Reduced['date_seconds'] <= window_end)\n",
    "        ].copy()\n",
    "\n",
    "        if trial_segment.empty:\n",
    "            continue\n",
    "\n",
    "        # Add trial-specific labels\n",
    "        trial_segment['Collider_Name'] = collider_name\n",
    "        trial_segment['Occurrence_Order'] = len(trials) + 1  # Increment trial count\n",
    "        trial_segment['Trial_ID'] = f\"{collider_name}_Trial_{len(trials) + 1}\"\n",
    "        trials.append(trial_segment)\n",
    "\n",
    "    # Combine all trials into a single DataFrame\n",
    "    if trials:\n",
    "        trials_df = pd.concat(trials, ignore_index=True)\n",
    "    else:\n",
    "        continue  # Skip this file if no trials are found\n",
    "\n",
    "    # Save trials_df\n",
    "    participant_id = file_path[-10:-4]\n",
    "    trials_df.to_csv(os.path.join(output_trials_dir, f\"{participant_id}_trials_df.csv\"), index=False)\n",
    "\n",
    "    # Calculate transition matrices and entropy\n",
    "    entropy_results = []\n",
    "    for trial_id, trial_data in trials_df.groupby('Trial_ID'):\n",
    "        collider_name = trial_data['Collider_Name'].iloc[0]\n",
    "        occurrence_order = trial_data['Occurrence_Order'].iloc[0]\n",
    "\n",
    "        gaze_sequence = trial_data['Collider_CategoricalN'].reset_index(drop=True)\n",
    "        categories = gaze_sequence.unique()\n",
    "        transition_matrix = pd.DataFrame(0, index=categories, columns=categories, dtype=float)\n",
    "\n",
    "        # Build the transition matrix\n",
    "        for i in range(len(gaze_sequence) - 1):\n",
    "            current_category = gaze_sequence.iloc[i]\n",
    "            next_category = gaze_sequence.iloc[i + 1]\n",
    "            transition_matrix.loc[current_category, next_category] += 1\n",
    "\n",
    "        # Normalize the transition matrix\n",
    "        transition_matrix = transition_matrix.div(transition_matrix.sum(axis=1), axis=0).fillna(0)\n",
    "\n",
    "        # Save transition matrix\n",
    "        transition_matrix.to_csv(os.path.join(output_transition_dir, f\"{participant_id}_transition_matrix.csv\"))\n",
    "\n",
    "        # Calculate stationary distribution\n",
    "        try:\n",
    "            eigvals, eigvecs = np.linalg.eig(transition_matrix.T)\n",
    "            stationary_distribution = np.real(eigvecs[:, np.isclose(eigvals, 1)].flatten())\n",
    "            stationary_distribution /= stationary_distribution.sum()\n",
    "\n",
    "            stationary_distribution_dict = {categories[i]: stationary_distribution[i] for i in range(len(categories))}\n",
    "        except:\n",
    "            stationary_distribution_dict = {category: 1 / len(categories) for category in categories}\n",
    "\n",
    "        # Calculate entropy\n",
    "        def calculate_transition_entropy(matrix, stationary_distribution):\n",
    "            total_entropy = 0\n",
    "            for i, row in matrix.iterrows():\n",
    "                row_entropy = sum(-p * log2(p) for p in row if p > 0)\n",
    "                total_entropy += row_entropy * stationary_distribution.get(i, 0)\n",
    "            return total_entropy\n",
    "\n",
    "        overall_transition_entropy = calculate_transition_entropy(transition_matrix, stationary_distribution_dict)\n",
    "\n",
    "        num_categories = len(transition_matrix)\n",
    "        normalized_overall_entropy = overall_transition_entropy / log2(num_categories) if num_categories > 1 else 0\n",
    "\n",
    "        result = {\n",
    "            'Trial_ID': trial_id,\n",
    "            'Collider_Name': collider_name,\n",
    "            'Occurrence_Order': occurrence_order,\n",
    "            'Overall_Transition_Entropy': normalized_overall_entropy\n",
    "        }\n",
    "        entropy_results.append(result)\n",
    "\n",
    "    # Save entropy results\n",
    "    entropy_df = pd.DataFrame(entropy_results)\n",
    "    entropy_df.to_csv(os.path.join(output_entropy_dir, f\"{participant_id}_entropy_results.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ce50fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
