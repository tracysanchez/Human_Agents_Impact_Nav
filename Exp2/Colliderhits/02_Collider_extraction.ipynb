{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6d06a85-751a-4e42-a434-a5c025194b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e99a4a4f-2972-43e6-9094-4408f5a082c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the collider names are too detailed, here we create a dictionary with patterns to classify them into our categories of interest\n",
    "\n",
    "patterns = {'\\d{2}_Sa':'Passive_Agent', '\\d{2}_Cma':'Active_Agent', 'Building_\\d+': 'Building'}\n",
    "patterns.update(dict.fromkeys(['Castle-TaskBuilding_56', 'Crane_59','HighSilo-TaskBuilding_49', 'Windmill-TaskBuilding_10_1', 'Church-TaskBuilding_16'], 'Global_Landmark'))\n",
    "patterns.update(dict.fromkeys(['^TaskBuilding_2$','^TaskBuilding_3$', '^TaskBuilding_5$', '^TaskBuilding_8$', '^TaskBuilding_9$', '^TaskBuilding_11$', '^TaskBuilding_13$', '^TaskBuilding_14$', '^TaskBuilding_20$', \n",
    "                               '^TaskBuilding_21$', '^TaskBuilding_23$','^TaskBuilding_27$', '^TaskBuilding_29$', '^TaskBuilding_32$', '^TaskBuilding_34$',  '^TaskBuilding_38$', '^TaskBuilding_41$', '^TaskBuilding_42$', \n",
    "                               '^TaskBuilding_44$', '^TaskBuilding_45$', '^TaskBuilding_47$', '^TaskBuilding_50$', '^TaskBuilding_51$', '^TaskBuilding_52$', 'BasketballCourt_58', 'Construction_57', \n",
    "                               '^Graffity_02$', '^Graffity_03$', '^Graffity_05$', '^Graffity_08$', '^Graffity_09$', '^Graffity_11$', '^Graffity_13$', '^Graffity_14$', '^Graffity_20$', \n",
    "                               '^Graffity_21$', '^Graffity_23$', '^Graffity_27$', '^Graffity_29$', '^Graffity_32$', '^Graffity_34$', '^Graffity_38$', '^Graffity_41$', '^Graffity_42$', \n",
    "                               '^Graffity_44$', '^Graffity_45$', '^Graffity_47$',  '^Graffity_50$', '^Graffity_51$', '^Graffity_52$'], 'TaskBuilding_Public'))\n",
    "\n",
    "patterns.update(dict.fromkeys(['^TaskBuilding_1$','^TaskBuilding_4$', '^TaskBuilding_6$', '^TaskBuilding_7$', '^TaskBuilding_12$', '^TaskBuilding_15$', '^TaskBuilding_17$', '^TaskBuilding_18$', '^TaskBuilding_19$', \n",
    "                               '^TaskBuilding_22$', '^TaskBuilding_24$','^TaskBuilding_25$', '^TaskBuilding_26$', '^TaskBuilding_28$', '^TaskBuilding_30$',  '^TaskBuilding_31$', '^TaskBuilding_33$', '^TaskBuilding_35$', \n",
    "                               '^TaskBuilding_36$', '^TaskBuilding_37$', '^TaskBuilding_39$', '^TaskBuilding_40$', '^TaskBuilding_43$', '^TaskBuilding_48$', '^TaskBuilding_54$','^TaskBuilding_55$',\n",
    "                               '^Graffity_01$','^Graffity_04$', '^Graffity_06$', '^Graffity_07$', '^Graffity_12$', '^Graffity_15$', '^Graffity_17$', '^Graffity_18$', '^Graffity_19$', '^Graffity_22$', \n",
    "                               '^Graffity_24$','^Graffity_25$', '^Graffity_26$', '^Graffity_28$', '^Graffity_30$',  '^Graffity_31$', '^Graffity_33$', '^Graffity_35$', '^Graffity_36$', '^Graffity_37$', '^Graffity_39$', \n",
    "                               '^Graffity_40$', '^Graffity_43$', '^Graffity_48$', '^Graffity_54$', '^Graffity_55$' ], 'TaskBuilding_Residential'))\n",
    "default_val = 'Background'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d1fc42a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/TwoTeras/1_Experiment_2/Eye_Tracking/Pre_processed/02_Individuals_Colliders/1031.csv\n",
      "/Volumes/TwoTeras/1_Experiment_2/Eye_Tracking/Pre_processed/02_Individuals_Colliders/1268.csv\n",
      "/Volumes/TwoTeras/1_Experiment_2/Eye_Tracking/Pre_processed/02_Individuals_Colliders/1574.csv\n",
      "/Volumes/TwoTeras/1_Experiment_2/Eye_Tracking/Pre_processed/02_Individuals_Colliders/1843.csv\n",
      "/Volumes/TwoTeras/1_Experiment_2/Eye_Tracking/Pre_processed/02_Individuals_Colliders/2069.csv\n",
      "/Volumes/TwoTeras/1_Experiment_2/Eye_Tracking/Pre_processed/02_Individuals_Colliders/3193.csv\n",
      "/Volumes/TwoTeras/1_Experiment_2/Eye_Tracking/Pre_processed/02_Individuals_Colliders/3540.csv\n",
      "/Volumes/TwoTeras/1_Experiment_2/Eye_Tracking/Pre_processed/02_Individuals_Colliders/4580.csv\n",
      "/Volumes/TwoTeras/1_Experiment_2/Eye_Tracking/Pre_processed/02_Individuals_Colliders/4598.csv\n",
      "/Volumes/TwoTeras/1_Experiment_2/Eye_Tracking/Pre_processed/02_Individuals_Colliders/4847.csv\n",
      "/Volumes/TwoTeras/1_Experiment_2/Eye_Tracking/Pre_processed/02_Individuals_Colliders/4875.csv\n",
      "/Volumes/TwoTeras/1_Experiment_2/Eye_Tracking/Pre_processed/02_Individuals_Colliders/5161.csv\n",
      "/Volumes/TwoTeras/1_Experiment_2/Eye_Tracking/Pre_processed/02_Individuals_Colliders/5189.csv\n",
      "/Volumes/TwoTeras/1_Experiment_2/Eye_Tracking/Pre_processed/02_Individuals_Colliders/5743.csv\n",
      "/Volumes/TwoTeras/1_Experiment_2/Eye_Tracking/Pre_processed/02_Individuals_Colliders/5766.csv\n",
      "/Volumes/TwoTeras/1_Experiment_2/Eye_Tracking/Pre_processed/02_Individuals_Colliders/5851.csv\n",
      "/Volumes/TwoTeras/1_Experiment_2/Eye_Tracking/Pre_processed/02_Individuals_Colliders/5972.csv\n",
      "/Volumes/TwoTeras/1_Experiment_2/Eye_Tracking/Pre_processed/02_Individuals_Colliders/6406.csv\n",
      "/Volumes/TwoTeras/1_Experiment_2/Eye_Tracking/Pre_processed/02_Individuals_Colliders/7081.csv\n",
      "/Volumes/TwoTeras/1_Experiment_2/Eye_Tracking/Pre_processed/02_Individuals_Colliders/7393.csv\n",
      "/Volumes/TwoTeras/1_Experiment_2/Eye_Tracking/Pre_processed/02_Individuals_Colliders/7823.csv\n",
      "/Volumes/TwoTeras/1_Experiment_2/Eye_Tracking/Pre_processed/02_Individuals_Colliders/7935.csv\n",
      "/Volumes/TwoTeras/1_Experiment_2/Eye_Tracking/Pre_processed/02_Individuals_Colliders/8629.csv\n",
      "/Volumes/TwoTeras/1_Experiment_2/Eye_Tracking/Pre_processed/02_Individuals_Colliders/9297.csv\n",
      "/Volumes/TwoTeras/1_Experiment_2/Eye_Tracking/Pre_processed/02_Individuals_Colliders/9627.csv\n",
      "/Volumes/TwoTeras/1_Experiment_2/Eye_Tracking/Pre_processed/02_Individuals_Colliders/1142.csv\n",
      "/Volumes/TwoTeras/1_Experiment_2/Eye_Tracking/Pre_processed/02_Individuals_Colliders/1234.csv\n",
      "/Volumes/TwoTeras/1_Experiment_2/Eye_Tracking/Pre_processed/02_Individuals_Colliders/6266.csv\n",
      "/Volumes/TwoTeras/1_Experiment_2/Eye_Tracking/Pre_processed/02_Individuals_Colliders/5191.csv\n"
     ]
    }
   ],
   "source": [
    "path = \"/Volumes/TwoTeras/1_Experiment_2/Eye_Tracking/Pre_processed/02_Individuals_Colliders\" \n",
    "\n",
    "  \n",
    "# csv files in the path\n",
    "files = glob.glob(path + \"/*.csv\")\n",
    "  \n",
    "# defining an empty list to store \n",
    "# content\n",
    "data_frame = pd.DataFrame()\n",
    "content = []\n",
    "  \n",
    "# checking all the csv files in the \n",
    "# specified path\n",
    "for filename in files:\n",
    "    \n",
    "    # reading content of csv file\n",
    "    # content.append(filename)\n",
    "    df = pd.read_csv(filename)\n",
    "    df.drop(['Unnamed: 0', 'rayCastHitsCombinedEyes', 'timeStampGetVerboseData', 'hitObjectColliderBoundsCenter', 'timeStampDataPointStart', \n",
    "             'bodyTrackerPosition.x', 'bodyTrackerPosition.y', 'bodyTrackerPosition.z', 'hmdPosition.x', 'hmdPosition.y',\n",
    "             'hmdPosition.z', 'hmdDirectionForward.x', 'hmdDirectionForward.y',\n",
    "             'hmdDirectionForward.z', 'hmdRotation.x', 'hmdRotation.y',\n",
    "             'hmdRotation.z', 'hmdDirectionUp.x', 'hmdDirectionUp.y',\n",
    "             'hmdDirectionUp.z', 'bodyTrackerRotation.x', 'bodyTrackerRotation.y','bodyTrackerRotation.z',],axis=1, inplace=True)\n",
    "    #Since we have two colliders hits per frame, we calculate the distance between each hit and the participant\n",
    "    df['Eucledian_distance'] = np.linalg.norm(df.loc[:, [\"hitPointOnObject_x\",\"hitPointOnObject_y\",\"hitPointOnObject_z\"]].values - df.loc[:, [\"playerBodyPosition.x\",\"playerBodyPosition.y\",\"playerBodyPosition.z\"]], axis=1)\n",
    "    # Here we look for the patterns contained in the dictionary and create the more general/informative variable Collider_Categorical\n",
    "    df['Collider_Categorical'] =  df['hitObjectColliderName'].apply(lambda x: next((val for key, val in patterns.items() if re.match(key, x)), default_val))\n",
    "    df['Previous_Euclidean_value'] = df['Eucledian_distance'].shift(1)\n",
    "    #Here we declare the conditions to choose between collider hits:\n",
    "    df['Collider_stays'] = (df[\"ordinalOfHit\"] == 2) & (df['Collider_Categorical'] != 'Background') & (df['Eucledian_distance'] <  df['Previous_Euclidean_value'])\n",
    "    df.reset_index(inplace=True)\n",
    "    #Drop all the second hit colliders that do not comply with the criteria\n",
    "    indexCollider = df[(df[\"ordinalOfHit\"] == 2) & (df['Collider_stays'] == False)].index\n",
    "    depleted_data = df.drop(index=indexCollider)\n",
    "    depleted_data.reset_index(inplace=True, drop=True)\n",
    "    #Take the index of all second colliders that will stay\n",
    "    indexColliderStays = depleted_data[depleted_data['Collider_stays'] == True].index\n",
    "    #We subtract one from that list of indexes because now is the first collider that has to go (so row directly on top)\n",
    "    indexColliderDelete = indexColliderStays - 1\n",
    "    depleted_data_1 = depleted_data.drop(indexColliderDelete)\n",
    "    #Create variable that differenciates body from face hits on agents\n",
    "    depleted_data_1[\"Face_Hits\"] = \"Not_Agent\"\n",
    "    mask_Face = ((depleted_data_1[\"Collider_Categorical\"].str.contains(pat=\"_Agent\")) & (depleted_data_1[\"hitColliderType\"] == \"UnityEngine.SphereCollider\"))\n",
    "    mask_Body = ((depleted_data_1[\"Collider_Categorical\"].str.contains(pat=\"_Agent\")) & (depleted_data_1[\"hitColliderType\"] == \"UnityEngine.BoxCollider\"))\n",
    "    depleted_data_1.loc[mask_Face, 'Face_Hits'] = \"Face\"\n",
    "    depleted_data_1.loc[mask_Body, 'Face_Hits'] = \"Body\"\n",
    "    content.append(depleted_data_1)\n",
    "    print(filename)\n",
    "  \n",
    "# converting content to data frame\n",
    "data_frame = pd.concat(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8651389f-2037-4fee-ba0f-15ca95a0da34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['level_0', 'index', 'SubjectID', 'Session', 'SessionSubsection',\n",
       "       'timeStampDataPointEnd', 'combinedGazeValidityBitmask',\n",
       "       'eyePositionCombinedWorld.x', 'eyePositionCombinedWorld.y',\n",
       "       'eyePositionCombinedWorld.z', 'eyeDirectionCombinedWorld.y',\n",
       "       'eyeDirectionCombinedWorld.z', 'eyeDirectionCombinedLocal.x',\n",
       "       'eyeDirectionCombinedLocal.y', 'eyeDirectionCombinedLocal.z',\n",
       "       'playerBodyPosition.x', 'playerBodyPosition.y', 'playerBodyPosition.z',\n",
       "       'hitColliderType', 'hitObjectColliderName', 'ordinalOfHit',\n",
       "       'hitPointOnObject_x', 'hitPointOnObject_y', 'hitPointOnObject_z',\n",
       "       'Eucledian_distance', 'Collider_Categorical',\n",
       "       'Previous_Euclidean_value', 'Collider_stays', 'Face_Hits'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63165a9d-0067-46b7-95c6-b92800969b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame.drop(['level_0', 'index','Collider_stays', 'Previous_Euclidean_value'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4cde1c7-932c-42b5-9dd0-40b2b4ab172a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame.to_csv(\"/Volumes/TwoTeras/1_Experiment_2/Eye_Tracking/Pre_processed/Data_Sets/Complete_data_Categorical_L.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc2d0f66-35d1-436d-9da4-530206b494a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Trash' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-b677259a4ef3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mTrash\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'Trash' is not defined"
     ]
    }
   ],
   "source": [
    "Trash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b97509",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Dealing with time \n",
    " \n",
    "- Create continues time \n",
    "- Delete duplicate time stamps\n",
    "- Create individual csv files per subject per session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc036f8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_frame =  pd.read_csv(\"/Volumes/TwoTeras/1_Experiment_2/Eye_Tracking/Pre_processed/Data_Sets/Complete_data_Categorical_L.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a74f4792",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Group by Subject and Session because the continuous time only makes sence on a session scale\n",
    "all_participants_sessions = pd.DataFrame(data_frame.groupby([\"SubjectID\", \"Session\"])[\"hitColliderType\"].count())\n",
    "all_participants_sessions.reset_index(inplace=True)\n",
    "# Create tuples to later subset data to subject-session scale. We create this because subjects can have a different number of sessions, in this way we only subset for the existing pairs S-S.\n",
    "L_all_participants_sessions = list(zip(all_participants_sessions.SubjectID, all_participants_sessions.Session))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aabf6faa",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(L_all_participants_sessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afdc76a1-86a0-45ae-9e96-b7fd831a72bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1031, 1)\n",
      "(1031, 2)\n",
      "(1031, 3)\n",
      "(1031, 4)\n",
      "(1031, 5)\n",
      "(1142, 1)\n",
      "(1142, 2)\n",
      "(1142, 3)\n",
      "(1142, 4)\n",
      "(1142, 5)\n",
      "(1234, 1)\n",
      "(1234, 2)\n",
      "(1234, 3)\n",
      "(1234, 4)\n",
      "(1234, 5)\n",
      "(1268, 1)\n",
      "(1268, 2)\n",
      "(1268, 3)\n",
      "(1268, 4)\n",
      "(1268, 5)\n",
      "(1574, 1)\n",
      "(1574, 2)\n",
      "(1574, 3)\n",
      "(1574, 4)\n",
      "(1574, 5)\n",
      "(1843, 1)\n",
      "(1843, 2)\n",
      "(1843, 3)\n",
      "(1843, 4)\n",
      "(1843, 5)\n",
      "(2069, 1)\n",
      "(2069, 2)\n",
      "(2069, 3)\n",
      "(2069, 4)\n",
      "(2069, 5)\n",
      "(3193, 1)\n",
      "(3193, 2)\n",
      "(3193, 3)\n",
      "(3193, 4)\n",
      "(3193, 5)\n",
      "(3540, 1)\n",
      "(3540, 2)\n",
      "(3540, 3)\n",
      "(3540, 4)\n",
      "(3540, 5)\n",
      "(4580, 1)\n",
      "(4580, 2)\n",
      "(4580, 3)\n",
      "(4580, 4)\n",
      "(4580, 5)\n",
      "(4598, 1)\n",
      "(4598, 2)\n",
      "(4598, 3)\n",
      "(4598, 4)\n",
      "(4598, 5)\n",
      "(4847, 1)\n",
      "(4847, 2)\n",
      "(4847, 3)\n",
      "(4847, 4)\n",
      "(4847, 5)\n",
      "(4875, 1)\n",
      "(4875, 2)\n",
      "(4875, 3)\n",
      "(4875, 4)\n",
      "(4875, 5)\n",
      "(5161, 1)\n",
      "(5161, 2)\n",
      "(5161, 3)\n",
      "(5161, 4)\n",
      "(5161, 5)\n",
      "(5189, 1)\n",
      "(5189, 2)\n",
      "(5189, 3)\n",
      "(5189, 4)\n",
      "(5189, 5)\n",
      "(5191, 1)\n",
      "(5191, 2)\n",
      "(5191, 3)\n",
      "(5191, 4)\n",
      "(5191, 5)\n",
      "(5743, 1)\n",
      "(5743, 2)\n",
      "(5743, 3)\n",
      "(5743, 4)\n",
      "(5743, 5)\n",
      "(5766, 1)\n",
      "(5766, 2)\n",
      "(5766, 3)\n",
      "(5766, 4)\n",
      "(5766, 5)\n",
      "(5851, 1)\n",
      "(5851, 2)\n",
      "(5851, 3)\n",
      "(5851, 4)\n",
      "(5851, 5)\n",
      "(5972, 1)\n",
      "(5972, 2)\n",
      "(5972, 3)\n",
      "(5972, 4)\n",
      "(5972, 5)\n",
      "(6266, 1)\n",
      "(6266, 2)\n",
      "(6266, 3)\n",
      "(6266, 4)\n",
      "(6266, 5)\n",
      "(6406, 1)\n",
      "(6406, 2)\n",
      "(6406, 3)\n",
      "(6406, 4)\n",
      "(6406, 5)\n",
      "(7081, 1)\n",
      "(7081, 2)\n",
      "(7081, 3)\n",
      "(7081, 4)\n",
      "(7081, 5)\n",
      "(7393, 1)\n",
      "(7393, 2)\n",
      "(7393, 3)\n",
      "(7393, 4)\n",
      "(7393, 5)\n",
      "(7823, 1)\n",
      "(7823, 2)\n",
      "(7823, 3)\n",
      "(7823, 4)\n",
      "(7823, 5)\n",
      "(7935, 1)\n",
      "(7935, 2)\n",
      "(7935, 3)\n",
      "(7935, 4)\n",
      "(7935, 5)\n",
      "(8629, 1)\n",
      "(8629, 2)\n",
      "(8629, 3)\n",
      "(8629, 4)\n",
      "(8629, 5)\n",
      "(9297, 1)\n",
      "(9297, 2)\n",
      "(9297, 3)\n",
      "(9297, 4)\n",
      "(9297, 5)\n",
      "(9627, 1)\n",
      "(9627, 2)\n",
      "(9627, 3)\n",
      "(9627, 4)\n",
      "(9627, 5)\n"
     ]
    }
   ],
   "source": [
    "temporalss = []\n",
    "for tuples in L_all_participants_sessions:\n",
    "    subject, session = tuples\n",
    "    temporal = data_frame[(data_frame['SubjectID'] == subject) & (data_frame['Session'] == session)]\n",
    "    temporal_c = temporal.sort_values(by=['timeStampDataPointEnd'])\n",
    "    temporal_cr = temporal_c.reset_index(drop=True)\n",
    "    temporal_cr[\"Time_Shift\"] = temporal_cr.timeStampDataPointEnd.diff()\n",
    "    temporal_c_ND =  temporal_cr[(temporal_cr[\"Time_Shift\"] > 0.001) & (temporal_cr[\"Time_Shift\"] < 1)].copy()\n",
    "    number = temporal_c_ND.columns.get_loc('timeStampDataPointEnd')\n",
    "    first_time= temporal_c_ND.iloc[0, number]\n",
    "    temporal_c_ND[\"Continuous_Time\"] = np.round(((temporal_c_ND.iloc[:, number] - first_time)/60), 3)\n",
    "    # Delete duplicate time stamps\n",
    "    temporal_c_ND.to_csv(f\"/Volumes/TwoTeras/1_Experiment_2/Eye_Tracking/Pre_processed/03_Individuals_IndividualSessions/{subject}_{session}.csv\", index=True)\n",
    "    temporalss.append(temporal_c_ND)\n",
    "    print(tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd39433-9938-4584-a2fe-a92bdd7e6967",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
