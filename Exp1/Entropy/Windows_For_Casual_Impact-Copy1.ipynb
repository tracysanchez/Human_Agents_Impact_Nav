{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "876772d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datetime import timedelta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4e55670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping dictionary\n",
    "mapping = {\n",
    "    \"TaskBuilding_Public\": \"Task_Building\",\n",
    "    \"TaskBuilding_Residential\": \"Task_Building\",\n",
    "    \"Active_Agent\": \"Agent\",\n",
    "    \"Passive_Agent\": \"Agent\",\n",
    "    \"Active_Agent_Face\": \"Agent\",\n",
    "    \"Passive_Agent_Face\": \"Agent\"\n",
    "}\n",
    "\n",
    "# Collider list\n",
    "collider_list = [\n",
    "    '56_Sa', '39_Sa', '19_Cma', '55_Sa', '25_Cma', '40_Sa', '41_Sa',\n",
    "    '17_Cma', '47_Sa', '03_Cma', '13_Cma', '24_Cma', '01_Cma', '54_Sa',\n",
    "    '15_Cma', '29_Sa', '04_Cma', '49_Sa', '30_Sa', '02_Cma', '51_Sa',\n",
    "    '08_Cma', '28_Cma', '26_Cma', '44_Sa', '06_Cma', '53_Sa', '37_Sa',\n",
    "    '32_Sa', '20_Cma', '16_Cma', '50_Sa', '34_Sa', '11_Cma', '38_Sa',\n",
    "    '33_Sa', '12_Cma', '22_Cma', '42_Sa', '05_Cma', '23_Cma', '18_Cma',\n",
    "    '27_Cma', '45_Sa', '43_Sa', '09_Cma', '31_Sa', '48_Sa', '10_Cma',\n",
    "    '52_Sa', '07_Cma', '46_Sa', '35_Sa', '36_Sa', '21_Cma', '14_Cma'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "585556aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate Chao-Shen entropy\n",
    "def chao_shen(q):\n",
    "    yx = q[q > 0]  # Remove zero-count bins\n",
    "    n = np.sum(yx)\n",
    "    p = yx.astype(float) / n\n",
    "    f1 = np.sum(yx == 1)\n",
    "    if f1 == n:\n",
    "        f1 -= 1\n",
    "    C = 1 - (f1 / n)  # Sample coverage\n",
    "    pa = C * p\n",
    "    la = 1 - (1 - pa) ** n\n",
    "    H = -np.sum((pa * np.log2(pa)) / la)\n",
    "    return H if n > 1 else 0  # Ensure valid entropy calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efde699-7152-4352-90b6-c47d14b7cc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute transition matrices using mapped categories\n",
    "def compute_transition_entropy(window_data):\n",
    "    if window_data.empty:\n",
    "        return np.nan\n",
    "    gaze_sequence = window_data['Mapped_Category'].reset_index(drop=True)\n",
    "    categories = gaze_sequence.unique()\n",
    "    if len(categories) < 2:\n",
    "        return np.nan\n",
    "    transition_matrix = pd.DataFrame(0, index=categories, columns=categories, dtype=float)\n",
    "    for i in range(len(gaze_sequence) - 1):\n",
    "        transition_matrix.loc[gaze_sequence.iloc[i], gaze_sequence.iloc[i + 1]] += 1\n",
    "    transition_sums = transition_matrix.sum(axis=1)\n",
    "    if transition_sums.sum() == 0:\n",
    "        return np.nan  # Avoid division errors\n",
    "    H = chao_shen(transition_sums.values)\n",
    "    return H / np.log2(len(categories)) if len(categories) > 1 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "209b52d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files: 100%|█████████████████████| 145/145 [04:19<00:00,  1.79s/file]\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "# Define paths\n",
    "input_dir = \"/Volumes/TwoTeras/0_Experiment_1/Eye_Tracking/Pre_processed/05_Debbies_gaze/\"\n",
    "output_entropy_dir = \"/Volumes/TwoTeras/0_Experiment_1/Entropy_Results/Window/entropy_results/CausalImpact/\"\n",
    "os.makedirs(output_entropy_dir, exist_ok=True)\n",
    "\n",
    "file_paths = glob.glob(os.path.join(input_dir, \"*.csv\"))\n",
    "\n",
    "# Process all CSV files\n",
    "for file_path in tqdm(file_paths, desc=\"Processing Files\", unit=\"file\"):\n",
    "    participant_id = os.path.basename(file_path).split('.')[0]\n",
    "    data = pd.read_csv(file_path)\n",
    "    data['date_seconds'] = pd.to_datetime(data['timeStampDataPointEnd'], unit='s')\n",
    "    \n",
    "    # Map categories\n",
    "    data['Mapped_Category'] = data['Collider_CategoricalN'].replace(mapping)\n",
    "    \n",
    "    # Identify gaze events on colliders and calculate occurrence order\n",
    "    data_reduced = data[data['events'] == -2]\n",
    "    filtered_df = data[(data['events'] == -2) & (data['names'].isin(collider_list))].copy()\n",
    "    filtered_df['Occurrence_Order'] = filtered_df.groupby('names').cumcount() + 1\n",
    "    \n",
    "    last_processed_time = {}  # Track last processed time per collider\n",
    "    results = []\n",
    "    for trial_index, (index, row) in enumerate(filtered_df.iterrows(), start=1):\n",
    "        gaze_time = row['date_seconds']\n",
    "        collider_name = row['names']\n",
    "        occurrence_order = row['Occurrence_Order']\n",
    "        trial_id = f\"{collider_name}_Trial_{trial_index}\"\n",
    "        \n",
    "        # Ensure no overlapping windows\n",
    "        if collider_name in last_processed_time and (gaze_time - last_processed_time[collider_name]).total_seconds() <= 30:\n",
    "            continue  # Skip overlapping trials\n",
    "        last_processed_time[collider_name] = gaze_time\n",
    "        \n",
    "        # Ensure window boundaries do not exceed data limits\n",
    "        pre_window_start = max(data_reduced['date_seconds'].min(), gaze_time - timedelta(seconds=30))\n",
    "        pre_window_end = min(gaze_time, data_reduced['date_seconds'].max())\n",
    "        post_window_start = max(gaze_time, data_reduced['date_seconds'].min())\n",
    "        post_window_end = min(data_reduced['date_seconds'].max(), gaze_time + timedelta(seconds=30))\n",
    "        \n",
    "        # Extract gaze sequences for both windows\n",
    "        pre_window_data = data_reduced[(data_reduced['date_seconds'] >= pre_window_start) & (data_reduced['date_seconds'] <= pre_window_end)]\n",
    "        post_window_data = data_reduced[(data_reduced['date_seconds'] >= post_window_start) & (data_reduced['date_seconds'] <= post_window_end)]\n",
    "        pre_entropy = compute_transition_entropy(pre_window_data)\n",
    "        post_entropy = compute_transition_entropy(post_window_data)\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'Participant_ID': participant_id,\n",
    "            'Trial_ID': trial_id,\n",
    "            'Collider_Name': collider_name,\n",
    "            'Occurrence_Order': occurrence_order,\n",
    "            'Gaze_Time': gaze_time,\n",
    "            'Pre_Entropy': pre_entropy,\n",
    "            'Post_Entropy': post_entropy\n",
    "        })\n",
    "    \n",
    "    # Save results for each participant separately\n",
    "    entropy_df = pd.DataFrame(results)\n",
    "    entropy_df.to_csv(os.path.join(output_entropy_dir, f\"CausalImpact_entropy_{participant_id}.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f39e7402",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files: 100%|█████████████████| 145/145 [00:00<00:00, 534423.62file/s]\n"
     ]
    }
   ],
   "source": [
    "for file_path in tqdm(file_paths, desc=\"Processing Files\", unit=\"file\"):\n",
    "    participant_id = os.path.basename(file_path).split('.')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1765647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'9586_5'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "participant_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46536d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6dae986",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
